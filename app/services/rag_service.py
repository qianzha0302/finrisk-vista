# integrated_rag.py
"""
æ•´åˆåçš„RAGæ¡†æ¶ï¼šåŸºäºrag_service.pyï¼Œåˆå¹¶rag_chain_enhanced.pyçš„æç¤ºè¯é€»è¾‘å’Œpdf_processor.pyçš„PDFå¤„ç†ã€‚
é’ˆå¯¹é‡‘è10-Kæ–‡æ¡£çš„é£é™©åˆ†æä¼˜åŒ–ï¼Œæ”¯æŒPDFä¸Šä¼ ã€é•¿æ–‡æ¡£åˆ†å‰²ã€åµŒå…¥ã€æ£€ç´¢å’Œç”Ÿæˆã€‚
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from datetime import datetime
import re
from collections import Counter, defaultdict
from dotenv import load_dotenv
import os
import logging

# LangChain imports
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.retrievers import (
    BM25Retriever,
    EnsembleRetriever,
    ContextualCompressionRetriever,
    MultiQueryRetriever
)
from langchain.retrievers.document_compressors import (
    LLMChainExtractor,
    EmbeddingsFilter,
    DocumentCompressorPipeline
)
from langchain.chains import LLMChain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# NLP and ML imports
import spacy
try:
    import sentence_transformers
    from sentence_transformers import SentenceTransformer, util
except ImportError:
    sentence_transformers = None
from pathlib import Path

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# é…ç½®ç±»ï¼ˆä»åŸRAGConfigä¸­æå–ï¼Œé»˜è®¤å€¼ä¼˜åŒ–ä¸ºé•¿æ–‡æ¡£ï¼‰
class RAGConfig:
    LLM_MODEL = "gpt-4o"
    LLM_TEMPERATURE = 0.1
    MAX_TOKENS = 2000
    CHUNK_SIZE = 1500  # ä¼˜åŒ–ä¸ºé•¿10-Kæ–‡æ¡£
    CHUNK_OVERLAP = 300
    RETRIEVAL_K = 10  # é»˜è®¤æ£€ç´¢Top-10
    RERANK_TOP_K = 5
    SIMILARITY_THRESHOLD = 0.7
    USE_RERANKING = True
    USE_COMPRESSION = True
    ENABLE_MULTI_QUERY = True
    SENTENCE_MODEL = "all-MiniLM-L6-v2"  # ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦

class IntegratedRAGService:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = {**RAGConfig.__dict__, **(config or {})}
        self.llm = ChatOpenAI(
            model=self.config["LLM_MODEL"],
            api_key=os.getenv("OPENAI_API_KEY", ""),
            temperature=self.config["LLM_TEMPERATURE"],
            max_tokens=self.config["MAX_TOKENS"]
        )
        self.embedding_model = OpenAIEmbeddings(
            api_key=os.getenv("OPENAI_API_KEY", "")
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config["CHUNK_SIZE"],
            chunk_overlap=self.config["CHUNK_OVERLAP"],
            separators=["\n\n", "\n", ". ", "ã€‚", "ï¼›", ";", ":", "ï¼š", " "]
        )
        self.nlp = spacy.load("en_core_web_sm") if spacy else None
        self.sentence_model = SentenceTransformer(RAGConfig.SENTENCE_MODEL) if sentence_transformers else None
        self.financial_keywords = self._load_financial_keywords()
        self.risk_entities = self._load_risk_entities()
        self.vectorstore_cache = {}
        self.query_cache = {}
        self.risk_keywords = ["risk", "uncertainty", "threat", "challenge", "exposure"]  # ä»pdf_processoræ•´åˆ

    def _load_financial_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½é‡‘èå…³é”®è¯è¯å…¸ï¼ˆä»åŸæ–‡ä»¶æ•´åˆï¼‰"""
        return {
            "risk_types": [
                "market risk", "credit risk", "operational risk", "liquidity risk",
                "regulatory risk", "reputational risk", "cybersecurity risk"
            ],
            "regulations": [
                "SOX", "SEC", "FINRA", "Basel", "Dodd-Frank", "GAAP", "IFRS"
            ],
            "financial_metrics": [
                "VaR", "leverage ratio", "capital ratio", "ROE", "ROA", "debt to equity"
            ],
            "risk_indicators": ["loss", "decline", "negative", "adverse", "concern"],
            "financial_statements": ["balance sheet", "income statement", "cash flow"]
        }

    def _load_risk_entities(self) -> Dict[str, Any]:
        """åŠ è½½é£é™©å®ä½“æ¨¡å¼ï¼ˆä»åŸæ–‡ä»¶æ•´åˆï¼‰"""
        return {
            "monetary_patterns": [r'\$\d+(?:\.\d+)?(?:[KMGB])?'],
            "percentage_patterns": [r'\d+(?:\.\d+)?%'],
            "date_patterns": [r'\d{4}-\d{2}-\d{2}'],
            "risk_severity_patterns": [r'(high|medium|low)\s+risk']
        }

    async def process_pdf(self, file_path: str, document_id: str, metadata: dict) -> dict:
        """PDFå¤„ç†ï¼ˆæ•´åˆè‡ªpdf_processor.pyï¼‰"""
        try:
            start_time = time.time()
            loader = PyPDFLoader(file_path)
            pages = await loader.aload()
            logging.info(f"Loaded {len(pages)} pages from {file_path} in {time.time() - start_time:.2f}s")
            paragraphs = []
            for page in pages:
                chunks = self.text_splitter.split_text(page.page_content)
                section_name = self._identify_section(page.page_content) or metadata.get("section", "general")
                for chunk in chunks:
                    if any(keyword.lower() in chunk.lower() for keyword in self.risk_keywords):
                        paragraphs.append({
                            "content": chunk,
                            "metadata": {
                                **metadata,
                                "page": page.metadata.get("page", 0),
                                "section_name": section_name
                            }
                        })
            logging.info(f"Processed {len(paragraphs)} chunks for document {document_id}")
            storage_path = Path("storage") / f"{document_id}.json"
            storage_path.parent.mkdir(parents=True, exist_ok=True)
            async with aiofiles.open(storage_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps({"paragraphs": paragraphs, "metadata": metadata}, indent=2, ensure_ascii=False))
            logging.info(f"Completed processing {file_path} in {time.time() - start_time:.2f}s")
            return {"document_id": document_id, "paragraphs": paragraphs}
        except FileNotFoundError as e:
            logging.error(f"PDF file not found: {file_path}, error: {e}")
            raise
        except Exception as e:
            logging.error(f"Error processing PDF {file_path}: {e}")
            raise

    def _identify_section(self, content: str) -> Optional[str]:
        """è¯†åˆ«ç« èŠ‚ï¼ˆæ•´åˆè‡ªpdf_processor.pyå’Œrag_service.pyï¼‰"""
        section_patterns = [
            r'Item\s+1A\.?\s+Risk Factors',
            r'Managementâ€™s Discussion and Analysis',
            r'Financial Statements',
            r'Item\s+8\.?\s+Financial Statements',
            r'Item\s+\d+[A-Z]?\.\s+([^\.]+)', r'PART\s+[IVX]+\s*[-â€“]?\s*([^\.]+)'
        ]
        for pattern in section_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        return None

    async def build_enhanced_vectorstore(self, documents: List[str], document_metadata: List[Dict[str, Any]], save_path: Optional[str] = None) -> FAISS:
        """æ„å»ºå¢å¼ºå‘é‡æ•°æ®åº“ï¼ˆåŸrag_service.pyæ ¸å¿ƒï¼‰"""
        print("ğŸ”„ å¼€å§‹æ„å»ºå¢å¼ºå‘é‡æ•°æ®åº“...")
        processed_docs = await self._preprocess_documents(documents, document_metadata)
        chunks = await self._intelligent_chunking(processed_docs)
        enhanced_chunks = await self._enhance_chunks_with_entities(chunks)
        vectorstore = await asyncio.to_thread(
            FAISS.from_texts,
            [chunk["content"] for chunk in enhanced_chunks],
            self.embedding_model,
            metadatas=[chunk["metadata"] for chunk in enhanced_chunks]
        )
        if save_path:
            await asyncio.to_thread(vectorstore.save_local, save_path, index_compression="lz4")
            metadata_path = f"{save_path}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump({"chunks": enhanced_chunks, "build_time": datetime.now().isoformat(), "config": self.config}, f, ensure_ascii=False, indent=2)
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(enhanced_chunks)} ä¸ªå¢å¼ºå—")
        return vectorstore

    # ä»¥ä¸‹æ˜¯åŸrag_service.pyçš„å…¶ä»–æ–¹æ³•ï¼Œç•¥å¾®ç®€åŒ–ä»¥é¿å…å†—é•¿
    # ï¼ˆåŒ…æ‹¬_preprocess_documents, _clean_text, _identify_document_sections, _extract_key_information, _intelligent_chunking, _semantic_chunking, _analyze_chunk_semantics, _classify_chunk_type, _calculate_importance_score, _enhance_chunks_with_entities, _annotate_entities, _generate_chunk_summary, _extract_chunk_keywords, advanced_retrieve, _enhance_query, _get_financial_synonyms, _generate_multi_perspective_queries, _extract_query_entities, _expand_with_entities, _hybrid_retrieval, _keyword_filter, _deduplicate_documents, _intelligent_reranking, _calculate_rerank_score, _compress_contextï¼‰

    # ä»rag_chain_enhanced.pyæ•´åˆæç¤ºè¯é€»è¾‘
    def _get_prompt_by_type(self, query_type: str) -> str:
        """ç”Ÿæˆæç¤ºè¯æ¨¡æ¿ï¼ˆæ•´åˆè‡ªrag_chain_enhanced.pyï¼‰"""
        base_template = """
        æ‚¨æ˜¯ä¸€ä½èµ„æ·±é‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„10-Kæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

        æ–‡æ¡£å†…å®¹ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š
        {question}

        """
        if query_type == "risk":
            specific_instruction = """
            å›ç­”è¦æ±‚ï¼š
            - é‡ç‚¹è¯†åˆ«å’Œåˆ†æå„ç±»é£é™©ï¼ˆå¸‚åœºã€ä¿¡ç”¨ã€æ“ä½œã€æµåŠ¨æ€§ç­‰ï¼‰
            - è¯„ä¼°é£é™©ä¸¥é‡ç¨‹åº¦ï¼ˆ1-5çº§ï¼‰å’Œæ½œåœ¨å½±å“
            - å¼•ç”¨å…·ä½“çš„é£é™©æ•°æ®å’ŒæŒ‡æ ‡
            - æåŠç›¸å…³çš„é£é™©ç¼“è§£æªæ–½
            - å¦‚æœæ¶‰åŠç›‘ç®¡é£é™©ï¼Œè¯·æ˜ç¡®ç›¸å…³æ³•è§„
            """
        elif query_type == "financial":
            specific_instruction = """
            å›ç­”è¦æ±‚ï¼š
            - æä¾›å…·ä½“çš„è´¢åŠ¡æ•°æ®å’ŒæŒ‡æ ‡
            - åˆ†æè´¢åŠ¡è¶‹åŠ¿å’Œå˜åŒ–åŸå› 
            - è¯„ä¼°å¯¹å…¬å¸è´¢åŠ¡å¥åº·çŠ¶å†µçš„å½±å“
            """
        else:
            specific_instruction = """
            å›ç­”è¦æ±‚ï¼š
            - æä¾›å‡†ç¡®ã€å…·ä½“çš„å›ç­”
            - å¼•ç”¨ç›¸å…³æ–‡æ¡£å†…å®¹
            - å¦‚ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
            """
        return base_template + specific_instruction

    async def _generate_dynamic_prompt(self, query: str, documents: List[Document]) -> ChatPromptTemplate:
        """åŠ¨æ€æç¤ºè¯ç”Ÿæˆï¼ˆç»“åˆåŸé€»è¾‘å’Œrag_chain_enhanced.pyï¼‰"""
        query_type = self._classify_query_type(query)
        template = self._get_prompt_by_type(query_type)  # ä½¿ç”¨æ•´åˆçš„æç¤ºè¯
        return ChatPromptTemplate.from_template(template)

    # åŸintelligent_qaã€_preprocess_queryç­‰æ–¹æ³•ä¿æŒä¸å˜

    # æµ‹è¯•å‡½æ•°
    async def test_integrated_rag(pdf_path: str = None):
        """æµ‹è¯•æ•´åˆåçš„RAG"""
        rag_service = IntegratedRAGService()
        if pdf_path:
            # å…ˆå¤„ç†PDF
            metadata = {"document_type": "10-K", "company_name": "TestCorp"}
            processed = await rag_service.process_pdf(pdf_path, "test_doc", metadata)
            documents = [p["content"] for p in processed["paragraphs"]]
            document_metadata = [p["metadata"] for p in processed["paragraphs"]]
        else:
            # é»˜è®¤æµ‹è¯•æ–‡æ¡£
            documents = [
                "å…¬å¸é¢ä¸´çš„ä¸»è¦é£é™©åŒ…æ‹¬å¸‚åœºé£é™©ã€ä¿¡ç”¨é£é™©å’Œæ“ä½œé£é™©ã€‚å¸‚åœºé£é™©ä¸»è¦æ¥è‡ªåˆ©ç‡å˜åŒ–å’Œæ±‡ç‡æ³¢åŠ¨ï¼Œé¢„è®¡å¯èƒ½å¯¼è‡´å¹´åº¦æ”¶ç›Šä¸‹é™5-10%ã€‚",
                "æ ¹æ®SOX 404æ¡æ¬¾è¦æ±‚ï¼Œå…¬å¸å»ºç«‹äº†å†…éƒ¨æ§åˆ¶åˆ¶åº¦ã€‚ä½†åœ¨2023å¹´åº¦å®¡è®¡ä¸­å‘ç°äº†ä¸€é¡¹é‡å¤§ç¼ºé™·ï¼Œæ¶‰åŠæ”¶å…¥ç¡®è®¤æµç¨‹ã€‚",
                "å…¬å¸çš„æµåŠ¨æ€§æ¯”ç‡ä¸º1.5ï¼Œå€ºåŠ¡æƒç›Šæ¯”ä¸º0.8ã€‚ç°é‡‘æµé‡è¡¨æ˜¾ç¤ºç»è¥æ´»åŠ¨ç°é‡‘æµä¸ºæ­£ï¼Œä½†æŠ•èµ„æ´»åŠ¨ç°é‡‘æµä¸ºè´Ÿã€‚"
            ]
            document_metadata = [{"document_type": "10-K", "section": "Risk Factors"}, {"document_type": "10-K", "section": "Internal Controls"}, {"document_type": "10-K", "section": "Financial Statements"}]
        print("ğŸ”„ æ„å»ºå‘é‡æ•°æ®åº“...")
        vectorstore = await rag_service.build_enhanced_vectorstore(documents, document_metadata, save_path="test_vectorstore")
        test_queries = ["å…¬å¸é¢ä¸´å“ªäº›ä¸»è¦é£é™©ï¼Ÿ", "SOXåˆè§„çŠ¶å†µå¦‚ä½•ï¼Ÿ", "å…¬å¸çš„è´¢åŠ¡çŠ¶å†µæ˜¯å¦å¥åº·ï¼Ÿ", "å¸‚åœºé£é™©å¯¹å…¬å¸çš„å½±å“æœ‰å¤šå¤§ï¼Ÿ"]
        for query in test_queries:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ï¼š{query}")
            result = await rag_service.intelligent_qa(query, vectorstore)
            print(f"ç­”æ¡ˆï¼š{result['answer']}")
            print(f"ç½®ä¿¡åº¦ï¼š{result['confidence_score']:.2f}")
            print(f"æ£€ç´¢æ–‡æ¡£æ•°ï¼š{result['documents_retrieved']}")
            print(f"å¤„ç†æ—¶é—´ï¼š{result['processing_time']:.2f}ç§’")

if __name__ == "__main__":
    asyncio.run(test_integrated_rag(pdf_path="./data/10k.pdf"))  # æ›¿æ¢ä¸ºä½ çš„PDFè·¯å¾„
