# services/rag_service.py
"""
ç»Ÿä¸€çš„RAGæœåŠ¡ - æ•´åˆEnhancedRAGChainå’ŒAdvancedRAGService
é’ˆå¯¹é‡‘èé£é™©åˆ†æä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
"""

import asyncio
import json
import time
import os
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from datetime import datetime
from collections import Counter, defaultdict

# LangChain imports
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.chains import RetrievalQA

# NLP imports
try:
    import spacy
    nlp_available = True
except ImportError:
    spacy = None
    nlp_available = False

try:
    from sentence_transformers import SentenceTransformer, util
    sentence_transformers_available = True
except ImportError:
    sentence_transformers_available = False

import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class UnifiedRAGService:
    """ç»Ÿä¸€çš„RAGæœåŠ¡ï¼Œæ•´åˆç®€å•å’Œé«˜çº§åŠŸèƒ½"""
    
    def __init__(self, config: Dict[str, Any] = None):
        # é»˜è®¤é…ç½®
        self.config = {
            "chunk_size": 1000,
            "chunk_overlap": 200,
            "llm_temperature": 0.1,
            "max_tokens": 2000,
            "retrieval_k": 10,
            "rerank_top_k": 5,
            "similarity_threshold": 0.7,
            "use_reranking": True,
            "use_compression": True,
            "enable_multi_query": True,
            "model_name": "gpt-4o"
        }
        
        # åˆå¹¶è‡ªå®šä¹‰é…ç½®
        if config:
            self.config.update(config)
        
        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model=self.config["model_name"],
            api_key=os.getenv("OPENAI_API_KEY", ""),
            temperature=self.config["llm_temperature"],
            max_tokens=self.config["max_tokens"]
        )
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedding_model = OpenAIEmbeddings(
            api_key=os.getenv("OPENAI_API_KEY", "")
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config["chunk_size"],
            chunk_overlap=self.config["chunk_overlap"],
            separators=["\n\n", "\n", ". ", "ã€‚", "ï¼›", ";", ":", "ï¼š", " "]
        )
        
        # åˆå§‹åŒ–NLPå·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.nlp = None
        if nlp_available:
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except Exception as e:
                logging.warning(f"æ— æ³•åŠ è½½spacyæ¨¡å‹: {e}")
        
        # åˆå§‹åŒ–sentence transformerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.sentence_model = None
        if sentence_transformers_available:
            try:
                self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
            except Exception as e:
                logging.warning(f"æ— æ³•åŠ è½½sentence transformer: {e}")
        
        # åŠ è½½é‡‘èå…³é”®è¯å’Œå®ä½“æ¨¡å¼
        self.financial_keywords = self._load_financial_keywords()
        self.risk_entities = self._load_risk_entities()
        
        # ç¼“å­˜
        self.vectorstore_cache = {}
        self.query_cache = {}

    def _load_financial_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½é‡‘èå…³é”®è¯è¯å…¸"""
        return {
            "risk_types": [
                "market risk", "credit risk", "operational risk", "liquidity risk",
                "regulatory risk", "reputational risk", "cybersecurity risk",
                "interest rate risk", "currency risk", "commodity risk"
            ],
            "regulations": [
                "SOX", "SEC", "FINRA", "Basel", "Dodd-Frank", "GAAP", "IFRS",
                "COSO", "PCAOB", "FDIC", "OCC"
            ],
            "financial_metrics": [
                "VaR", "leverage ratio", "capital ratio", "ROE", "ROA", "debt to equity",
                "current ratio", "quick ratio", "gross margin", "net margin", "EBITDA"
            ],
            "risk_indicators": [
                "risk", "uncertainty", "threat", "challenge", "exposure", "vulnerability",
                "adverse", "negative", "decline", "loss", "deficit", "concern", "issue"
            ],
            "financial_statements": [
                "balance sheet", "income statement", "cash flow statement", "statement of equity",
                "10-K", "10-Q", "8-K", "annual report", "quarterly report"
            ]
        }

    def _load_risk_entities(self) -> Dict[str, Any]:
        """åŠ è½½é£é™©å®ä½“è¯†åˆ«æ¨¡å¼"""
        return {
            "monetary_patterns": [
                r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:million|billion|thousand|M|B|K))?',
                r'(?:USD|dollars?)\s*[\d,]+(?:\.\d{2})?',
                r'[\d,]+(?:\.\d{2})?\s*(?:million|billion|thousand)\s*(?:dollars?|USD)?'
            ],
            "percentage_patterns": [
                r'\d+(?:\.\d+)?%',
                r'\d+(?:\.\d+)?\s*percent',
                r'percentage\s*of\s*\d+(?:\.\d+)?'
            ],
            "date_patterns": [
                r'\d{4}',  # å¹´ä»½
                r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s*\d{1,2},?\s*\d{4}',
                r'\d{1,2}/\d{1,2}/\d{4}',
                r'\d{4}-\d{2}-\d{2}'
            ],
            "risk_severity_patterns": [
                r'(?:high|medium|low|significant|material|substantial)\s*(?:risk|exposure|threat)',
                r'(?:critical|severe|moderate|minor)\s*(?:risk|impact|concern)'
            ]
        }

    async def build_enhanced_vectorstore(
        self, 
        documents: List[str], 
        document_metadata: List[Dict[str, Any]], 
        save_path: Optional[str] = None
    ) -> FAISS:
        """æ„å»ºå¢å¼ºçš„å‘é‡æ•°æ®åº“"""
        logging.info("ğŸ”„ å¼€å§‹æ„å»ºå¢å¼ºå‘é‡æ•°æ®åº“...")
        
        try:
            # é¢„å¤„ç†æ–‡æ¡£
            processed_docs = await self._preprocess_documents(documents, document_metadata)
            
            # æ™ºèƒ½åˆ†å—
            chunks = await self._intelligent_chunking(processed_docs)
            
            # å¢å¼ºchunks
            enhanced_chunks = await self._enhance_chunks_with_entities(chunks)
            
            # æ„å»ºå‘é‡æ•°æ®åº“
            vectorstore = await asyncio.to_thread(
                FAISS.from_texts,
                [chunk["content"] for chunk in enhanced_chunks],
                self.embedding_model,
                metadatas=[chunk["metadata"] for chunk in enhanced_chunks]
            )
            
            # ä¿å­˜åˆ°æœ¬åœ°ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
            if save_path:
                await asyncio.to_thread(vectorstore.save_local, save_path)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata_path = f"{save_path}_metadata.json"
                metadata_info = {
                    "chunks": enhanced_chunks,
                    "build_time": datetime.now().isoformat(),
                    "config": self.config,
                    "total_chunks": len(enhanced_chunks),
                    "documents_count": len(documents)
                }
                
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata_info, f, ensure_ascii=False, indent=2)
            
            logging.info(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(enhanced_chunks)} ä¸ªå¢å¼ºå—")
            return vectorstore
            
        except Exception as e:
            logging.error(f"æ„å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise

    async def _preprocess_documents(self, documents: List[str], metadata: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ–‡æ¡£é¢„å¤„ç†"""
        processed = []
        
        for i, (doc, meta) in enumerate(zip(documents, metadata)):
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self._clean_text(doc)
            
            # è¯†åˆ«æ–‡æ¡£æ®µè½
            sections = self._identify_document_sections(cleaned_text)
            
            # æå–å…³é”®ä¿¡æ¯
            key_info = self._extract_key_information(cleaned_text)
            
            processed_doc = {
                "content": cleaned_text,
                "metadata": {
                    **meta,
                    "sections": sections,
                    "key_info": key_info,
                    "document_index": i,
                    "processed_at": datetime.now().isoformat(),
                    "word_count": len(cleaned_text.split())
                }
            }
            processed.append(processed_doc)
        
        return processed

    def _clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…ç†"""
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # æ ‡å‡†åŒ–å¼•å·
        text = text.replace('"', '"').replace('"', '"').replace(''', "'").replace(''', "'")
        
        # ç§»é™¤é¡µç 
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'\d+\s*$', '', text, flags=re.MULTILINE)
        
        # ä¿®æ­£å¸¸è§OCRé”™è¯¯
        text = text.replace('l0', '10').replace('O0', '00')
        
        return text.strip()

    def _identify_document_sections(self, text: str) -> List[str]:
        """è¯†åˆ«æ–‡æ¡£ç« èŠ‚"""
        sections = []
        
        # å®šä¹‰ç« èŠ‚è¯†åˆ«æ¨¡å¼
        section_patterns = [
            r'Item\s+\d+[A-Z]?\.\s+([^\.]{10,100})',  # SEC Item sections
            r'PART\s+[IVX]+\s*[-â€“]?\s*([^\.]{10,100})',  # Part sections
            r'(?:^|\n)\s*(\d+\.\s+[^\.]{10,100})',  # Numbered sections
            r'(?:^|\n)\s*([A-Z][A-Z\s]{10,50})\s*(?:\n|$)'  # All caps headers
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            sections.extend([match.strip() for match in matches if len(match.strip()) > 5])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(set(sections))[:15]

    def _extract_key_information(self, text: str) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        key_info = {
            "monetary_amounts": [],
            "percentages": [],
            "dates": [],
            "risk_mentions": [],
            "regulatory_references": []
        }
        
        # æå–è´§å¸é‡‘é¢
        for pattern in self.risk_entities["monetary_patterns"]:
            matches = re.findall(pattern, text, re.IGNORECASE)
            key_info["monetary_amounts"].extend(matches[:10])
        
        # æå–ç™¾åˆ†æ¯”
        for pattern in self.risk_entities["percentage_patterns"]:
            matches = re.findall(pattern, text, re.IGNORECASE)
            key_info["percentages"].extend(matches[:15])
        
        # æå–æ—¥æœŸ
        for pattern in self.risk_entities["date_patterns"]:
            matches = re.findall(pattern, text)
            key_info["dates"].extend(matches[:10])
        
        # è¯†åˆ«é£é™©ç±»å‹
        text_lower = text.lower()
        for risk_type in self.financial_keywords["risk_types"]:
            if risk_type in text_lower:
                key_info["risk_mentions"].append(risk_type)
        
        # è¯†åˆ«ç›‘ç®¡å¼•ç”¨
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in text_lower:
                key_info["regulatory_references"].append(regulation)
        
        return key_info

    async def _intelligent_chunking(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½åˆ†å—ç­–ç•¥"""
        all_chunks = []
        
        for doc in documents:
            content = doc["content"]
            metadata = doc["metadata"]
            
            # åŸºç¡€åˆ†å—
            base_chunks = self.text_splitter.split_text(content)
            
            # è¯­ä¹‰å¢å¼ºåˆ†å—
            semantic_chunks = await self._semantic_chunking(base_chunks, metadata)
            all_chunks.extend(semantic_chunks)
        
        return all_chunks

    async def _semantic_chunking(self, chunks: List[str], base_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å—"""
        enhanced_chunks = []
        
        for i, chunk in enumerate(chunks):
            # åˆ†æè¯­ä¹‰ç‰¹å¾
            semantic_features = await self._analyze_chunk_semantics(chunk)
            
            # åˆ†ç±»chunkç±»å‹
            chunk_type = self._classify_chunk_type(chunk, semantic_features)
            
            # è®¡ç®—é‡è¦æ€§åˆ†æ•°
            importance_score = self._calculate_importance_score(chunk, semantic_features)
            
            enhanced_chunk = {
                "content": chunk,
                "metadata": {
                    **base_metadata,
                    "chunk_index": i,
                    "chunk_type": chunk_type,
                    "importance_score": importance_score,
                    "semantic_features": semantic_features,
                    "word_count": len(chunk.split()),
                    "has_numbers": bool(re.search(r'\d', chunk)),
                    "has_risk_keywords": any(
                        keyword in chunk.lower() 
                        for keyword_list in self.financial_keywords.values() 
                        for keyword in keyword_list
                    )
                }
            }
            enhanced_chunks.append(enhanced_chunk)
        
        return enhanced_chunks

    async def _analyze_chunk_semantics(self, chunk: str) -> Dict[str, Any]:
        """åˆ†æchunkçš„è¯­ä¹‰ç‰¹å¾"""
        features = {
            "entities": [],
            "risk_signals": 0,
            "financial_terms": 0,
            "regulatory_mentions": 0,
            "sentiment_indicators": []
        }
        
        # ä½¿ç”¨spacyæå–å®ä½“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.nlp:
            try:
                doc = self.nlp(chunk)
                features["entities"] = [
                    {"text": ent.text, "label": ent.label_} 
                    for ent in doc.ents 
                    if ent.label_ in ["ORG", "MONEY", "PERCENT", "DATE", "LAW", "PERSON"]
                ]
            except Exception as e:
                logging.warning(f"å®ä½“æå–å¤±è´¥: {e}")
        
        chunk_lower = chunk.lower()
        
        # è®¡ç®—é£é™©ä¿¡å·
        for risk_indicator in self.financial_keywords["risk_indicators"]:
            if risk_indicator in chunk_lower:
                features["risk_signals"] += 1
        
        # è®¡ç®—é‡‘èæœ¯è¯­
        for financial_term in self.financial_keywords["financial_metrics"]:
            if financial_term in chunk_lower:
                features["financial_terms"] += 1
        
        # è®¡ç®—ç›‘ç®¡æåŠ
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in chunk_lower:
                features["regulatory_mentions"] += 1
        
        # æƒ…æ„ŸæŒ‡æ ‡
        negative_indicators = ["risk", "loss", "decline", "decrease", "negative", "adverse", "concern", "issue", "problem", "threat"]
        positive_indicators = ["improve", "increase", "growth", "positive", "strong", "effective", "successful", "opportunity"]
        
        for indicator in negative_indicators:
            if indicator in chunk_lower:
                features["sentiment_indicators"].append(("negative", indicator))
        
        for indicator in positive_indicators:
            if indicator in chunk_lower:
                features["sentiment_indicators"].append(("positive", indicator))
        
        return features

    def _classify_chunk_type(self, chunk: str, features: Dict[str, Any]) -> str:
        """åˆ†ç±»chunkç±»å‹"""
        chunk_lower = chunk.lower()
        
        # é£é™©æŠ«éœ²
        if any(word in chunk_lower for word in ["risk factor", "item 1a", "risk management", "risk assessment"]):
            return "risk_disclosure"
        
        # è´¢åŠ¡æ•°æ®
        elif any(word in chunk_lower for word in ["financial statement", "balance sheet", "income statement", "cash flow"]):
            return "financial_data"
        
        # åˆè§„ç›¸å…³
        elif any(word in chunk_lower for word in ["internal control", "sox", "compliance", "audit"]):
            return "compliance"
        
        # ç®¡ç†å±‚åˆ†æ
        elif any(word in chunk_lower for word in ["management discussion", "md&a", "outlook", "forward-looking"]):
            return "management_analysis"
        
        # ç›‘ç®¡ç›¸å…³
        elif features["regulatory_mentions"] > 0:
            return "regulatory"
        
        # é£é™©è¯„ä¼°
        elif features["risk_signals"] > 2:
            return "risk_assessment"
        
        # è´¢åŠ¡æŒ‡æ ‡
        elif features["financial_terms"] > 1:
            return "financial_metrics"
        
        else:
            return "general"

    def _calculate_importance_score(self, chunk: str, features: Dict[str, Any]) -> float:
        """è®¡ç®—chunké‡è¦æ€§åˆ†æ•°"""
        score = 0.0
        
        # åŸºäºé•¿åº¦çš„åˆ†æ•°
        word_count = len(chunk.split())
        if 50 <= word_count <= 200:
            score += 0.2
        elif word_count > 200:
            score += 0.1
        
        # åŸºäºé£é™©ä¿¡å·çš„åˆ†æ•°
        score += min(features["risk_signals"] / 10, 0.3)
        
        # åŸºäºé‡‘èæœ¯è¯­çš„åˆ†æ•°
        score += min(features["financial_terms"] / 10, 0.2)
        
        # åŸºäºç›‘ç®¡æåŠçš„åˆ†æ•°
        score += min(features["regulatory_mentions"] / 5, 0.2)
        
        # åŸºäºå®ä½“æ•°é‡çš„åˆ†æ•°
        score += min(len(features["entities"]) / 10, 0.1)
        
        return min(score, 1.0)

    async def _enhance_chunks_with_entities(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å®ä½“ä¿¡æ¯å¢å¼ºchunks"""
        enhanced = []
        
        for chunk_data in chunks:
            chunk = chunk_data["content"]
            metadata = chunk_data["metadata"]
            
            # ç”Ÿæˆæ‘˜è¦
            chunk_summary = await self._generate_chunk_summary(chunk)
            
            # æå–å…³é”®è¯
            keywords = self._extract_chunk_keywords(chunk)
            
            enhanced_chunk = {
                "content": chunk,
                "metadata": {
                    **metadata,
                    "summary": chunk_summary,
                    "keywords": keywords,
                    "enhanced_at": datetime.now().isoformat()
                }
            }
            enhanced.append(enhanced_chunk)
        
        return enhanced

    async def _generate_chunk_summary(self, chunk: str) -> str:
        """ç”Ÿæˆchunkæ‘˜è¦"""
        if len(chunk) < 200:
            return chunk[:100] + "..."
        
        try:
            summary_prompt = ChatPromptTemplate.from_template("""
            è¯·ä¸ºä»¥ä¸‹é‡‘èæ–‡æ¡£ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ï¼š
            æ–‡æ¡£ç‰‡æ®µï¼š{chunk}
            æ‘˜è¦ï¼š
            """)
            
            chain = summary_prompt | self.llm | StrOutputParser()
            summary = await asyncio.to_thread(chain.invoke, {"chunk": chunk[:1000]})
            return summary.strip()
            
        except Exception as e:
            logging.warning(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return chunk[:100] + "..."

    def _extract_chunk_keywords(self, chunk: str) -> List[str]:
        """æå–chunkå…³é”®è¯"""
        keywords = []
        chunk_lower = chunk.lower()
        
        # ä»é¢„å®šä¹‰è¯å…¸æå–å…³é”®è¯
        for category, word_list in self.financial_keywords.items():
            for word in word_list:
                if word in chunk_lower:
                    keywords.append(word)
        
        # ä½¿ç”¨NLPæå–å®ä½“å…³é”®è¯
        if self.nlp:
            try:
                doc = self.nlp(chunk)
                for ent in doc.ents:
                    if ent.label_ in ["ORG", "MONEY", "PERCENT", "LAW", "PERSON"]:
                        keywords.append(ent.text.lower())
            except Exception:
                pass
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(set(keywords))[:10]

    # ===== æŸ¥è¯¢ç›¸å…³æ–¹æ³• =====

    async def intelligent_qa(
        self, 
        query: str, 
        vectorstore: FAISS, 
        conversation_history: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = await self._preprocess_query(query, conversation_history)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = await self._advanced_retrieve(processed_query, vectorstore)
            
            # å¤šè·³æ¨ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self._requires_multi_hop_reasoning(processed_query):
                relevant_docs = await self._multi_hop_retrieval(processed_query, vectorstore, relevant_docs)
            
            # ç”ŸæˆåŠ¨æ€æç¤ºè¯
            qa_prompt = await self._generate_dynamic_prompt(processed_query, relevant_docs)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = await self._generate_answer(qa_prompt, processed_query, relevant_docs)
            
            # åå¤„ç†ç­”æ¡ˆ
            final_answer = await self._post_process_answer(answer, processed_query, relevant_docs)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence_score = self._calculate_confidence(processed_query, relevant_docs, final_answer)
            
            # ç”Ÿæˆå¼•ç”¨
            citations = self._generate_citations(relevant_docs)
            
            # ç”Ÿæˆè§£é‡Š
            explanation = await self._generate_explanation(processed_query, final_answer, relevant_docs)
            
            processing_time = time.time() - start_time
            
            return {
                "query": query,
                "answer": final_answer,
                "confidence_score": confidence_score,
                "citations": citations,
                "explanation": explanation,
                "relevant_documents": [
                    {
                        "content": doc.page_content[:200] + "...",
                        "metadata": doc.metadata,
                        "relevance_score": doc.metadata.get("rerank_score", 0)
                    }
                    for doc in relevant_docs
                ],
                "processing_time": processing_time,
                "retrieval_strategy": "advanced_hybrid",
                "documents_retrieved": len(relevant_docs)
            }
            
        except Exception as e:
            logging.error(f"æ™ºèƒ½é—®ç­”å¤±è´¥: {e}")
            return {
                "query": query,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence_score": 0.0,
                "citations": [],
                "explanation": "ç³»ç»Ÿé”™è¯¯",
                "relevant_documents": [],
                "processing_time": time.time() - start_time,
                "retrieval_strategy": "error",
                "documents_retrieved": 0
            }

    async def _preprocess_query(self, query: str, conversation_history: List[Dict[str, str]] = None) -> str:
        """æŸ¥è¯¢é¢„å¤„ç†"""
        processed = query.strip()
        
        # æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡
        if conversation_history:
            context = self._extract_conversation_context(conversation_history)
            if context:
                processed = f"{context} {processed}"
        
        # æ‰©å±•é‡‘èæœ¯è¯­
        expanded = await self._expand_financial_terms(processed)
        
        return expanded

    def _extract_conversation_context(self, history: List[Dict[str, str]]) -> str:
        """æå–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if not history:
            return ""
        
        recent_context = []
        for item in history[-2:]:  # åªå–æœ€è¿‘2è½®å¯¹è¯
            if item.get("role") == "user":
                recent_context.append(f"ä¹‹å‰é—®é¢˜ï¼š{item.get('content', '')}")
        
        return " ".join(recent_context)

    async def _expand_financial_terms(self, query: str) -> str:
        """æ‰©å±•é‡‘èæœ¯è¯­"""
        query_lower = query.lower()
        expansions = []
        
        # é£é™©ç›¸å…³æ‰©å±•
        if "risk" in query_lower:
            risk_context = [
                rt for rt in self.financial_keywords["risk_types"] 
                if any(word in query_lower for word in rt.split())
            ]
            if risk_context:
                expansions.extend(risk_context[:3])
        
        # ç›‘ç®¡ç›¸å…³æ‰©å±•
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in query_lower:
                expansions.append(f"{regulation} compliance")
        
        if expansions:
            return f"{query} ({' OR '.join(expansions)})"
        
        return query

    async def _advanced_retrieve(self, query: str, vectorstore: FAISS) -> List[Document]:
        """é«˜çº§æ£€ç´¢ç­–ç•¥"""
        logging.info(f"ğŸ” å¼€å§‹é«˜çº§æ£€ç´¢ï¼š{query[:50]}...")
        
        try:
            # åŸºç¡€å‘é‡ç›¸ä¼¼æ€§æœç´¢
            documents = await asyncio.to_thread(
                vectorstore.similarity_search, 
                query, 
                k=self.config["retrieval_k"]
            )
            
            # å…³é”®è¯è¿‡æ»¤
            filtered_docs = self._keyword_filter(query, documents)
            
            # æ™ºèƒ½é‡æ’åº
            if self.config["use_reranking"] and len(filtered_docs) > self.config["rerank_top_k"]:
                reranked_docs = await self._intelligent_reranking(query, filtered_docs)
            else:
                reranked_docs = filtered_docs
            
            # ä¸Šä¸‹æ–‡å‹ç¼©
            if self.config["use_compression"] and len(reranked_docs) > 3:
                compressed_docs = await self._compress_context(query, reranked_docs)
            else:
                compressed_docs = reranked_docs
            
            logging.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(compressed_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            return compressed_docs
            
        except Exception as e:
            logging.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _keyword_filter(self, query: str, docs: List[Document]) -> List[Document]:
        """åŸºäºå…³é”®è¯è¿‡æ»¤æ–‡æ¡£"""
        query_terms = set(query.lower().split())
        filtered_docs = []
        
        for doc in docs:
            content_lower = doc.page_content.lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = sum(1 for term in query_terms if term in content_lower) / len(query_terms)
            
            # è®¡ç®—é‡‘èæœ¯è¯­åŒ¹é…åˆ†æ•°
            financial_score = 0
            for category, terms in self.financial_keywords.items():
                for term in terms:
                    if term in content_lower and any(qt in term for qt in query_terms):
                        financial_score += 1
            
            # ç»¼åˆåˆ†æ•°
            total_score = keyword_score * 0.7 + min(financial_score / 10, 1.0) * 0.3
            
            if total_score > self.config["similarity_threshold"] * 0.5:
                doc.metadata["keyword_score"] = keyword_score
                doc.metadata["financial_score"] = financial_score
                doc.metadata["total_score"] = total_score
                filtered_docs.append(doc)
        
        # æŒ‰åˆ†æ•°æ’åº
        return sorted(filtered_docs, key=lambda x: x.metadata.get("total_score", 0), reverse=True)

    async def _intelligent_reranking(self, query: str, documents: List[Document]) -> List[Document]:
        """æ™ºèƒ½é‡æ’åº"""
        reranked = []
        
        for doc in documents:
            rerank_score = await self._calculate_rerank_score(query, doc)
            doc.metadata["rerank_score"] = rerank_score
            reranked.append(doc)
        
        # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
        reranked.sort(key=lambda x: x.metadata.get("rerank_score", 0), reverse=True)
        
        return reranked[:self.config["rerank_top_k"]]

    async def _calculate_rerank_score(self, query: str, document: Document) -> float:
        """è®¡ç®—é‡æ’åºåˆ†æ•°"""
        score = 0.0
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰sentence transformerï¼‰
        if self.sentence_model:
            try:
                query_embedding = self.sentence_model.encode([query])
                doc_embedding = self.sentence_model.encode([document.page_content[:500]])
                semantic_similarity = util.cos_sim(query_embedding, doc_embedding)[0][0].item()
                score += semantic_similarity * 0.4
            except Exception:
                score += 0.2  # é»˜è®¤åˆ†æ•°
        else:
            score += 0.2
        
        # å…³é”®è¯åŒ¹é…åˆ†æ•°
        keyword_score = document.metadata.get("keyword_score", 0)
        score += keyword_score * 0.3
        
        # é‡è¦æ€§åˆ†æ•°
        importance_score = document.metadata.get("importance_score", 0.5)
        score += importance_score * 0.2
        
        # æ–‡æ¡£ç±»å‹æƒé‡
        chunk_type = document.metadata.get("chunk_type", "general")
        type_weights = {
            "risk_disclosure": 1.0,
            "compliance": 0.9,
            "financial_data": 0.8,
            "management_analysis": 0.7,
            "regulatory": 0.8,
            "risk_assessment": 0.9,
            "general": 0.5
        }
        score += type_weights.get(chunk_type, 0.5) * 0.1
        
        return min(score, 1.0)

    async def _compress_context(self, query: str, documents: List[Document]) -> List[Document]:
        """ä¸Šä¸‹æ–‡å‹ç¼©"""
        if len(documents) <= 3:
            return documents
        
        compression_prompt = ChatPromptTemplate.from_template("""
        ä½œä¸ºé‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µä¸­æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚
        ä¿æŒåŸæ–‡çš„é‡è¦ç»†èŠ‚ï¼Œä½†å»é™¤å†—ä½™å†…å®¹ã€‚
        
        æŸ¥è¯¢ï¼š{query}
        æ–‡æ¡£å†…å®¹ï¼š{context}
        
        è¯·æå–æœ€ç›¸å…³çš„ä¿¡æ¯ï¼ˆä¿æŒåŸæ–‡è¡¨è¿°ï¼‰ï¼š
        """)
        
        compressed_docs = []
        for doc in documents:
            try:
                chain = compression_prompt | self.llm | StrOutputParser()
                compressed_content = await asyncio.to_thread(
                    chain.invoke, 
                    {"query": query, "context": doc.page_content[:1500]}
                )
                
                compressed_doc = Document(
                    page_content=compressed_content.strip(),
                    metadata={
                        **doc.metadata,
                        "compressed": True,
                        "original_length": len(doc.page_content),
                        "compressed_length": len(compressed_content)
                    }
                )
                compressed_docs.append(compressed_doc)
                
            except Exception as e:
                logging.warning(f"å‹ç¼©å¤±è´¥ï¼Œä¿ç•™åŸæ–‡æ¡£: {e}")
                compressed_docs.append(doc)
        
        return compressed_docs

    def _requires_multi_hop_reasoning(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè·³æ¨ç†"""
        multi_hop_indicators = [
            "compare", "relationship", "impact", "cause", "effect", 
            "correlation", "trend", "change", "difference", "connection"
        ]
        query_lower = query.lower()
        return any(indicator in query_lower for indicator in multi_hop_indicators)

    async def _multi_hop_retrieval(self, query: str, vectorstore: FAISS, initial_docs: List[Document]) -> List[Document]:
        """å¤šè·³æ£€ç´¢"""
        # ä»åˆå§‹æ–‡æ¡£ä¸­æå–å…³é”®æ¦‚å¿µ
        key_concepts = await self._extract_key_concepts_from_docs(initial_docs)
        
        additional_docs = []
        for concept in key_concepts[:3]:  # åªå–å‰3ä¸ªæ¦‚å¿µ
            concept_query = f"{concept} {query}"
            try:
                concept_docs = await asyncio.to_thread(vectorstore.similarity_search, concept_query, k=3)
                additional_docs.extend(concept_docs)
            except Exception as e:
                logging.warning(f"æ¦‚å¿µæŸ¥è¯¢å¤±è´¥ {concept}: {e}")
        
        # åˆå¹¶å¹¶å»é‡
        all_docs = initial_docs + additional_docs
        return self._deduplicate_documents(all_docs)[:self.config["rerank_top_k"]]

    async def _extract_key_concepts_from_docs(self, docs: List[Document]) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–å…³é”®æ¦‚å¿µ"""
        key_concepts = []
        
        for doc in docs[:3]:  # åªåˆ†æå‰3ä¸ªæ–‡æ¡£
            content_lower = doc.page_content.lower()
            
            # æå–é‡‘èå…³é”®è¯
            for category, terms in self.financial_keywords.items():
                for term in terms:
                    if term in content_lower:
                        key_concepts.append(term)
        
        # ç»Ÿè®¡å¹¶è¿”å›æœ€å¸¸è§çš„æ¦‚å¿µ
        concept_counts = Counter(key_concepts)
        return [concept for concept, count in concept_counts.most_common(5)]

    def _deduplicate_documents(self, docs: List[Document]) -> List[Document]:
        """æ–‡æ¡£å»é‡"""
        seen_content = set()
        unique_docs = []
        
        for doc in docs:
            # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦ä½œä¸ºå†…å®¹ç­¾å
            content_signature = doc.page_content[:100]
            if content_signature not in seen_content:
                seen_content.add(content_signature)
                unique_docs.append(doc)
        
        return unique_docs

    async def _generate_dynamic_prompt(self, query: str, documents: List[Document]) -> ChatPromptTemplate:
        """åŠ¨æ€ç”Ÿæˆæç¤ºè¯"""
        # åˆ†ç±»æŸ¥è¯¢ç±»å‹
        query_type = self._classify_query_type(query)
        
        # åˆ†ææ–‡æ¡£ç±»å‹
        doc_types = [doc.metadata.get("chunk_type", "general") for doc in documents]
        dominant_doc_type = Counter(doc_types).most_common(1)[0][0] if doc_types else "general"
        
        # é€‰æ‹©åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
        prompt_template = self._select_prompt_template(query_type, dominant_doc_type)
        
        return ChatPromptTemplate.from_template(prompt_template)

    def _classify_query_type(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["risk", "exposure", "threat", "vulnerability"]):
            return "risk_analysis"
        elif any(word in query_lower for word in ["compliance", "regulation", "sox", "sec"]):
            return "compliance"
        elif any(word in query_lower for word in ["financial", "revenue", "profit", "loss", "metric"]):
            return "financial_analysis"
        elif any(word in query_lower for word in ["compare", "difference", "vs", "versus"]):
            return "comparison"
        elif any(word in query_lower for word in ["trend", "change", "over time", "historical"]):
            return "trend_analysis"
        else:
            return "general"

    def _select_prompt_template(self, query_type: str, doc_type: str) -> str:
        """é€‰æ‹©é€‚åˆçš„æç¤ºè¯æ¨¡æ¿"""
        base_template = """
æ‚¨æ˜¯ä¸€ä½èµ„æ·±çš„é‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„10-Kæ–‡æ¡£åˆ†æç»éªŒã€‚
è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç›¸å…³æ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

å›ç­”è¦æ±‚ï¼š
"""
        
        if query_type == "risk_analysis":
            base_template += """- é‡ç‚¹åˆ†æé£é™©ç±»å‹ã€ä¸¥é‡ç¨‹åº¦å’Œæ½œåœ¨å½±å“
- å¼•ç”¨å…·ä½“çš„é£é™©å› ç´ å’Œæ•°æ®
- è¯„ä¼°é£é™©çš„å¯æ§æ€§å’Œç¼“è§£æªæ–½
- å¦‚æœæ¶‰åŠç›‘ç®¡é£é™©ï¼Œè¯·æ˜ç¡®ç›¸å…³æ³•è§„æ¡æ¬¾
"""
        elif query_type == "compliance":
            base_template += """- æ˜ç¡®æŒ‡å‡ºç›¸å…³çš„ç›‘ç®¡è¦æ±‚å’Œåˆè§„çŠ¶æ€
- å¼•ç”¨å…·ä½“çš„æ³•è§„æ¡æ¬¾ï¼ˆå¦‚SOX 404ã€SEC Item 105ç­‰ï¼‰
- åˆ†æåˆè§„ç¼ºé™·çš„ä¸¥é‡æ€§å’Œæ•´æ”¹è¦æ±‚
- è¯„ä¼°å¯¹ä¸šåŠ¡è¿è¥çš„å½±å“
"""
        elif query_type == "financial_analysis":
            base_template += """- æä¾›å…·ä½“çš„è´¢åŠ¡æ•°æ®å’ŒæŒ‡æ ‡
- åˆ†æè´¢åŠ¡è¶‹åŠ¿å’Œå˜åŒ–åŸå› 
- è¯„ä¼°å¯¹å…¬å¸è´¢åŠ¡å¥åº·çŠ¶å†µçš„å½±å“
- å¦‚æœ‰åŒæ¯”æ•°æ®ï¼Œè¯·è¿›è¡Œå¯¹æ¯”åˆ†æ
"""
        elif query_type == "comparison":
            base_template += """- è¿›è¡Œè¯¦ç»†çš„å¯¹æ¯”åˆ†æ
- çªå‡ºæ˜¾ç¤ºå…³é”®å·®å¼‚å’Œç›¸ä¼¼ç‚¹
- åˆ†æå·®å¼‚çš„åŸå› å’Œå½±å“
- æä¾›æ•°æ®æ”¯æŒçš„ç»“è®º
"""
        else:
            base_template += """- æä¾›å‡†ç¡®ã€å…·ä½“çš„å›ç­”
- å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£å†…å®¹ä½œä¸ºä¾æ®
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
- ä¿æŒå®¢è§‚å’Œä¸“ä¸šçš„åˆ†æè§’åº¦
"""
        
        base_template += """
æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
- å¼•ç”¨æ—¶è¯·ä¿æŒåŸæ–‡çš„å‡†ç¡®æ€§
- æä¾›å¯ä¿¡åº¦è¯„ä¼°ï¼ˆé«˜/ä¸­/ä½ï¼‰
- å¦‚éœ€è¦ï¼Œå¯ä»¥æå‡ºè¿›ä¸€æ­¥åˆ†æçš„å»ºè®®

å›ç­”ï¼š
"""
        return base_template

    async def _generate_answer(self, prompt: ChatPromptTemplate, query: str, documents: List[Document]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc.page_content}" 
            for i, doc in enumerate(documents[:5])  # åªä½¿ç”¨å‰5ä¸ªæ–‡æ¡£
        ])
        
        try:
            chain = prompt | self.llm | StrOutputParser()
            answer = await asyncio.to_thread(
                chain.invoke, 
                {"context": context, "question": query}
            )
            return answer.strip()
            
        except Exception as e:
            logging.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"

    async def _post_process_answer(self, answer: str, query: str, documents: List[Document]) -> str:
        """ç­”æ¡ˆåå¤„ç†"""
        # éªŒè¯äº‹å®
        verified_answer = await self._verify_facts(answer, documents)
        
        # æ ¼å¼åŒ–ç­”æ¡ˆ
        formatted_answer = self._format_answer(verified_answer)
        
        # æ·»åŠ å…è´£å£°æ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self._requires_disclaimer(query):
            formatted_answer += "\n\nâš ï¸ å…è´£å£°æ˜ï¼šæœ¬åˆ†æåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œä»…ä¾›å‚è€ƒã€‚å…·ä½“å†³ç­–è¯·å’¨è¯¢ä¸“ä¸šé¡¾é—®ã€‚"
        
        return formatted_answer

    async def _verify_facts(self, answer: str, documents: List[Document]) -> str:
        """éªŒè¯ç­”æ¡ˆä¸­çš„äº‹å®"""
        # æå–ç­”æ¡ˆä¸­çš„æ•°å­—
        numbers = re.findall(r'\d+(?:\.\d+)?%?', answer)
        
        # æ£€æŸ¥è¿™äº›æ•°å­—æ˜¯å¦åœ¨åŸæ–‡æ¡£ä¸­
        all_doc_content = " ".join([doc.page_content for doc in documents])
        verified_numbers = [num for num in numbers if num in all_doc_content]
        
        # è®¡ç®—éªŒè¯æ¯”ä¾‹
        verification_ratio = len(verified_numbers) / len(numbers) if numbers else 1.0
        
        if verification_ratio < 0.5:
            answer = f"âš ï¸ éƒ¨åˆ†ä¿¡æ¯å¯èƒ½éœ€è¦è¿›ä¸€æ­¥éªŒè¯\n\n{answer}"
        
        return answer

    def _format_answer(self, answer: str) -> str:
        """æ ¼å¼åŒ–ç­”æ¡ˆ"""
        formatted = answer
        
        # ä¸ºé•¿ç­”æ¡ˆæ·»åŠ æ®µè½åˆ†éš”
        if len(formatted) > 500:
            formatted = re.sub(r'ã€‚([^ã€‚]{100,})', r'ã€‚\n\n\1', formatted)
        
        return formatted

    def _requires_disclaimer(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ å…è´£å£°æ˜"""
        disclaimer_triggers = [
            "investment", "invest", "buy", "sell", "recommendation", 
            "advice", "should", "suggest", "recommend"
        ]
        query_lower = query.lower()
        return any(trigger in query_lower for trigger in disclaimer_triggers)

    def _calculate_confidence(self, query: str, documents: List[Document], answer: str) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        confidence = 0.0
        
        # æ–‡æ¡£è´¨é‡åˆ†æ•°
        doc_quality = np.mean([doc.metadata.get("rerank_score", 0.5) for doc in documents])
        confidence += doc_quality * 0.4
        
        # ç­”æ¡ˆå®Œæ•´æ€§åˆ†æ•°
        answer_completeness = min(len(answer) / 500, 1.0)
        confidence += answer_completeness * 0.2
        
        # å…³é”®è¯åŒ¹é…åˆ†æ•°
        query_terms = set(query.lower().split())
        answer_terms = set(answer.lower().split())
        keyword_match = len(query_terms.intersection(answer_terms)) / len(query_terms)
        confidence += keyword_match * 0.2
        
        # æ–‡æ¡£æ•°é‡åˆ†æ•°
        doc_count_score = min(len(documents) / 5, 1.0)
        confidence += doc_count_score * 0.1
        
        # å…·ä½“æ•°æ®åˆ†æ•°
        has_specific_data = bool(re.search(r'\d+(?:\.\d+)?[%$]?', answer))
        if has_specific_data:
            confidence += 0.1
        
        return min(confidence, 1.0)

    def _generate_citations(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¼•ç”¨ä¿¡æ¯"""
        citations = []
        
        for i, doc in enumerate(documents):
            citation = {
                "id": i + 1,
                "content_preview": doc.page_content[:150] + "...",
                "relevance_score": doc.metadata.get("rerank_score", 0),
                "document_type": doc.metadata.get("chunk_type", "unknown"),
                "metadata": {
                    k: v for k, v in doc.metadata.items() 
                    if k in ["document_index", "chunk_index", "importance_score"]
                }
            }
            citations.append(citation)
        
        return citations

    async def _generate_explanation(self, query: str, answer: str, documents: List[Document]) -> str:
        """ç”Ÿæˆå›ç­”è§£é‡Š"""
        explanation_prompt = ChatPromptTemplate.from_template("""
è¯·ä¸ºä»¥ä¸‹é—®ç­”å¯¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„è§£é‡Šï¼Œè¯´æ˜ç­”æ¡ˆçš„ä¾æ®å’Œæ¨ç†è¿‡ç¨‹ï¼š

é—®é¢˜ï¼š{query}
ç­”æ¡ˆï¼š{answer}
ä½¿ç”¨çš„æ–‡æ¡£æ•°é‡ï¼š{doc_count}

è§£é‡Šï¼ˆä¸è¶…è¿‡100å­—ï¼‰ï¼š
""")
        
        try:
            chain = explanation_prompt | self.llm | StrOutputParser()
            explanation = await asyncio.to_thread(
                chain.invoke, 
                {
                    "query": query, 
                    "answer": answer[:500], 
                    "doc_count": len(documents)
                }
            )
            return explanation.strip()
            
        except Exception as e:
            logging.warning(f"è§£é‡Šç”Ÿæˆå¤±è´¥: {e}")
            return f"åŸºäº {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆçš„ç­”æ¡ˆ"

    # ===== ç®€å•RAGé“¾æ–¹æ³•ï¼ˆå…¼å®¹åŸæœ‰EnhancedRAGChainï¼‰ =====

    def build_smart_rag_chain(self, vectorstore, query_type: str = "general") -> RetrievalQA:
        """æ„å»ºæ™ºèƒ½RAGé“¾ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        try:
            # ç”Ÿæˆæç¤ºè¯æ¨¡æ¿
            prompt_template = self._get_prompt_by_type(query_type)
            
            # åˆ›å»ºæ£€ç´¢å™¨
            retriever = vectorstore.as_retriever(search_kwargs={"k": self.config["retrieval_k"]})
            
            # æ„å»ºRAGé“¾
            rag_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                retriever=retriever,
                chain_type="stuff",
                chain_type_kwargs={"prompt": prompt_template},
                return_source_documents=True
            )
            
            return rag_chain
            
        except Exception as e:
            logging.error(f"æ„å»ºRAGé“¾å¤±è´¥: {e}")
            raise

    def _get_prompt_by_type(self, query_type: str) -> ChatPromptTemplate:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·å–æç¤ºè¯ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        base_template = """
æ‚¨æ˜¯ä¸€ä½èµ„æ·±é‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„10-Kæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}
"""
        
        if query_type == "risk":
            specific_instruction = """
å›ç­”è¦æ±‚ï¼š
- é‡ç‚¹è¯†åˆ«å’Œåˆ†æå„ç±»é£é™©ï¼ˆå¸‚åœºã€ä¿¡ç”¨ã€æ“ä½œã€æµåŠ¨æ€§ç­‰ï¼‰
- è¯„ä¼°é£é™©ä¸¥é‡ç¨‹åº¦ï¼ˆ1-5çº§ï¼‰å’Œæ½œåœ¨å½±å“
- å¼•ç”¨å…·ä½“çš„é£é™©æ•°æ®å’ŒæŒ‡æ ‡
- æåŠç›¸å…³çš„é£é™©ç¼“è§£æªæ–½
- å¦‚æœæ¶‰åŠç›‘ç®¡é£é™©ï¼Œè¯·æ˜ç¡®ç›¸å…³æ³•è§„
"""
        elif query_type == "financial":
            specific_instruction = """
å›ç­”è¦æ±‚ï¼š
- æä¾›å…·ä½“çš„è´¢åŠ¡æ•°æ®å’ŒæŒ‡æ ‡
- åˆ†æè´¢åŠ¡è¶‹åŠ¿å’Œå˜åŒ–åŸå› 
- è¯„ä¼°å¯¹å…¬å¸è´¢åŠ¡å¥åº·çŠ¶å†µçš„å½±å“
"""
        else:
            specific_instruction = """
å›ç­”è¦æ±‚ï¼š
- æä¾›å‡†ç¡®ã€å…·ä½“çš„å›ç­”
- å¼•ç”¨ç›¸å…³æ–‡æ¡£å†…å®¹
- å¦‚ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
"""
        
        return ChatPromptTemplate.from_template(base_template + specific_instruction)


# æµ‹è¯•å‡½æ•°
async def test_unified_rag():
    """æµ‹è¯•ç»Ÿä¸€RAGç³»ç»Ÿ"""
    rag_service = UnifiedRAGService()
    
    test_documents = [
        "å…¬å¸é¢ä¸´çš„ä¸»è¦é£é™©åŒ…æ‹¬å¸‚åœºé£é™©ã€ä¿¡ç”¨é£é™©å’Œæ“ä½œé£é™©ã€‚å¸‚åœºé£é™©ä¸»è¦æ¥è‡ªåˆ©ç‡å˜åŒ–å’Œæ±‡ç‡æ³¢åŠ¨ï¼Œé¢„è®¡å¯èƒ½å¯¼è‡´å¹´åº¦æ”¶ç›Šä¸‹é™5-10%ã€‚",
        "æ ¹æ®SOX 404æ¡æ¬¾è¦æ±‚ï¼Œå…¬å¸å»ºç«‹äº†å†…éƒ¨æ§åˆ¶åˆ¶åº¦ã€‚ä½†åœ¨2023å¹´åº¦å®¡è®¡ä¸­å‘ç°äº†ä¸€é¡¹é‡å¤§ç¼ºé™·ï¼Œæ¶‰åŠæ”¶å…¥ç¡®è®¤æµç¨‹ã€‚",
        "å…¬å¸çš„æµåŠ¨æ€§æ¯”ç‡ä¸º1.5ï¼Œå€ºåŠ¡æƒç›Šæ¯”ä¸º0.8ã€‚ç°é‡‘æµé‡è¡¨æ˜¾ç¤ºç»è¥æ´»åŠ¨ç°é‡‘æµä¸ºæ­£ï¼Œä½†æŠ•èµ„æ´»åŠ¨ç°é‡‘æµä¸ºè´Ÿã€‚"
    ]
    
    test_metadata = [
        {"document_type": "10-K", "section": "Risk Factors"},
        {"document_type": "10-K", "section": "Internal Controls"},
        {"document_type": "10-K", "section": "Financial Statements"}
    ]
    
    print("ğŸ”„ æ„å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = await rag_service.build_enhanced_vectorstore(
        test_documents, 
        test_metadata, 
        save_path="test_vectorstore"
    )
    
    test_queries = [
        "å…¬å¸é¢ä¸´å“ªäº›ä¸»è¦é£é™©ï¼Ÿ",
        "SOXåˆè§„çŠ¶å†µå¦‚ä½•ï¼Ÿ",
        "å…¬å¸çš„è´¢åŠ¡çŠ¶å†µæ˜¯å¦å¥åº·ï¼Ÿ",
        "å¸‚åœºé£é™©å¯¹å…¬å¸çš„å½±å“æœ‰å¤šå¤§ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ï¼š{query}")
        result = await rag_service.intelligent_qa(query, vectorstore)
        print(f"ç­”æ¡ˆï¼š{result['answer']}")
        print(f"ç½®ä¿¡åº¦ï¼š{result['confidence_score']:.2f}")
        print(f"æ£€ç´¢æ–‡æ¡£æ•°ï¼š{result['documents_retrieved']}")
        print(f"å¤„ç†æ—¶é—´ï¼š{result['processing_time']:.2f}ç§’")


if __name__ == "__main__":
    asyncio.run(test_unified_rag())
