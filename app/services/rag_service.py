# services/advanced_rag_service.py
"""
é«˜çº§RAGæœåŠ¡ - é’ˆå¯¹é‡‘èé£é™©åˆ†æä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
Features:
- æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯+è¯­ä¹‰ï¼‰
- æ™ºèƒ½é‡æ’åº
- ä¸Šä¸‹æ–‡å‹ç¼©
- å¤šè·³æ¨ç†
- å®ä½“è¯†åˆ«å’Œå…³è”
- åŠ¨æ€æç¤ºè¯ç”Ÿæˆ
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from datetime import datetime

# LangChain imports
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.retrievers import (
    BM25Retriever,
    EnsembleRetriever,
    ContextualCompressionRetriever,
    MultiQueryRetriever
)
from langchain.retrievers.document_compressors import (
    LLMChainExtractor,
    EmbeddingsFilter,
    DocumentCompressorPipeline
)
from langchain.chains import LLMChain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# NLP and ML imports
import spacy
try:
    import sentence_transformers
    from sentence_transformers import SentenceTransformer, util
except ImportError:
    sentence_transformers = None
import re
from collections import Counter, defaultdict
from dotenv import load_dotenv
import os
from ..utils.rag_config import RAGConfig

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

class AdvancedRAGService:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = {**RAGConfig.get_config(), **(config or {})}  # åˆå¹¶é»˜è®¤å’Œè‡ªå®šä¹‰é…ç½®
        self.llm = ChatOpenAI(
            model=RAGConfig.LLM_MODEL,
            api_key=os.getenv("OPENAI_API_KEY", ""),
            temperature=self.config["llm_temperature"],
            max_tokens=self.config["max_tokens"]
        )
        self.embedding_model = OpenAIEmbeddings(
            api_key=os.getenv("OPENAI_API_KEY", "")
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config["chunk_size"],
            chunk_overlap=self.config["chunk_overlap"],
            separators=["\n\n", "\n", ". ", "ã€‚", "ï¼›", ";", ":", "ï¼š", " "]
        )
        self.nlp = spacy.load("en_core_web_sm") if spacy else None
        self.sentence_model = SentenceTransformer(RAGConfig.SENTENCE_MODEL) if sentence_transformers else None
        self.financial_keywords = self._load_financial_keywords()
        self.risk_entities = self._load_risk_entities()
        self.vectorstore_cache = {}
        self.query_cache = {}

    def _load_financial_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½é‡‘èå…³é”®è¯è¯å…¸"""
        return {
            "risk_types": [...],  # ä¿æŒåŸåˆ—è¡¨ï¼Œç•¥å»ä»¥èŠ‚çœç©ºé—´
            "financial_metrics": [...],
            "regulations": [...],
            "risk_indicators": [...],
            "financial_statements": [...]
        }

    def _load_risk_entities(self) -> Dict[str, Any]:
        """åŠ è½½é£é™©å®ä½“æ¨¡å¼"""
        return {
            "monetary_patterns": [...],  # ä¿æŒåŸåˆ—è¡¨
            "percentage_patterns": [...],
            "date_patterns": [...],
            "risk_severity_patterns": [...]
        }

    async def build_enhanced_vectorstore(self, documents: List[str], document_metadata: List[Dict[str, Any]], save_path: Optional[str] = None) -> FAISS:
        """æ„å»ºå¢å¼ºçš„å‘é‡æ•°æ®åº“"""
        print("ğŸ”„ å¼€å§‹æ„å»ºå¢å¼ºå‘é‡æ•°æ®åº“...")
        processed_docs = await self._preprocess_documents(documents, document_metadata)
        chunks = await self._intelligent_chunking(processed_docs)
        enhanced_chunks = await self._enhance_chunks_with_entities(chunks)
        vectorstore = await asyncio.to_thread(
            FAISS.from_texts,
            [chunk["content"] for chunk in enhanced_chunks],
            self.embedding_model,
            metadatas=[chunk["metadata"] for chunk in enhanced_chunks]
        )
        if save_path:
            await asyncio.to_thread(vectorstore.save_local, save_path)
            metadata_path = f"{save_path}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump({"chunks": enhanced_chunks, "build_time": datetime.now().isoformat(), "config": self.config}, f, ensure_ascii=False, indent=2)
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(enhanced_chunks)} ä¸ªå¢å¼ºå—")
        return vectorstore

    async def _preprocess_documents(self, documents: List[str], metadata: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ–‡æ¡£é¢„å¤„ç†"""
        processed = []
        for i, (doc, meta) in enumerate(zip(documents, metadata)):
            cleaned_text = self._clean_text(doc)
            sections = self._identify_document_sections(cleaned_text)
            key_info = self._extract_key_information(cleaned_text)
            processed.append({"content": cleaned_text, "metadata": {**meta, "sections": sections, "key_info": key_info, "document_index": i, "processed_at": datetime.now().isoformat()}})
        return processed

    def _clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…ç†"""
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('"', '"').replace('"', '"').replace(''', "'").replace(''', "'")
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'\d+\s*$', '', text, flags=re.MULTILINE)
        text = text.replace('l0', '10').replace('O0', '00')
        return text.strip()

    def _identify_document_sections(self, text: str) -> List[str]:
        """è¯†åˆ«æ–‡æ¡£ç« èŠ‚"""
        sections = []
        section_patterns = [r'Item\s+\d+[A-Z]?\.\s+([^\.]+)', r'PART\s+[IVX]+\s*[-â€“]?\s*([^\.]+)', r'(?:^|\n)\s*(\d+\.\s+[^\.]+)', r'(?:^|\n)\s*([A-Z][^\.]{5,50})\s*(?:\n|$)']
        for pattern in section_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            sections.extend([match.strip() for match in matches if len(match.strip()) > 3])
        return list(set(sections))[:10]

    def _extract_key_information(self, text: str) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        key_info = {"monetary_amounts": [], "percentages": [], "dates": [], "risk_mentions": [], "regulatory_references": []}
        for pattern in self.risk_entities["monetary_patterns"]:
            matches = re.findall(pattern, text, re.IGNORECASE)
            key_info["monetary_amounts"].extend(matches[:5])
        for pattern in self.risk_entities["percentage_patterns"]:
            matches = re.findall(pattern, text)
            key_info["percentages"].extend(matches[:10])
        for pattern in self.risk_entities["date_patterns"]:
            matches = re.findall(pattern, text)
            key_info["dates"].extend(matches[:5])
        text_lower = text.lower()
        for risk_type in self.financial_keywords["risk_types"]:
            if risk_type in text_lower:
                key_info["risk_mentions"].append(risk_type)
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in text_lower:
                key_info["regulatory_references"].append(regulation)
        return key_info

    async def _intelligent_chunking(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½åˆ†å—ç­–ç•¥"""
        all_chunks = []
        for doc in documents:
            content = doc["content"]
            metadata = doc["metadata"]
            base_chunks = self.text_splitter.split_text(content)
            semantic_chunks = await self._semantic_chunking(base_chunks, metadata)
            all_chunks.extend(semantic_chunks)
        return all_chunks

    async def _semantic_chunking(self, chunks: List[str], base_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å—"""
        enhanced_chunks = []
        for i, chunk in enumerate(chunks):
            semantic_features = await self._analyze_chunk_semantics(chunk)
            chunk_type = self._classify_chunk_type(chunk, semantic_features)
            importance_score = self._calculate_importance_score(chunk, semantic_features)
            enhanced_chunks.append({
                "content": chunk,
                "metadata": {
                    **base_metadata,
                    "chunk_index": i,
                    "chunk_type": chunk_type,
                    "importance_score": importance_score,
                    "semantic_features": semantic_features,
                    "word_count": len(chunk.split()),
                    "has_numbers": bool(re.search(r'\d', chunk)),
                    "has_risk_keywords": any(keyword in chunk.lower() for keyword_list in self.financial_keywords.values() for keyword in keyword_list)
                }
            })
        return enhanced_chunks

    async def _analyze_chunk_semantics(self, chunk: str) -> Dict[str, Any]:
        """åˆ†æchunkçš„è¯­ä¹‰ç‰¹å¾"""
        features = {"entities": [], "risk_signals": 0, "financial_terms": 0, "regulatory_mentions": 0, "sentiment_indicators": []}
        if self.nlp:
            doc = self.nlp(chunk)
            features["entities"] = [{"text": ent.text, "label": ent.label_} for ent in doc.ents if ent.label_ in ["ORG", "MONEY", "PERCENT", "DATE", "LAW"]]
        chunk_lower = chunk.lower()
        for risk_indicator in self.financial_keywords["risk_indicators"]:
            if risk_indicator in chunk_lower:
                features["risk_signals"] += 1
        for financial_term in self.financial_keywords["financial_metrics"]:
            if financial_term in chunk_lower:
                features["financial_terms"] += 1
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in chunk_lower:
                features["regulatory_mentions"] += 1
        negative_indicators = ["risk", "loss", "decline", "decrease", "negative", "adverse", "concern", "issue", "problem"]
        positive_indicators = ["improve", "increase", "growth", "positive", "strong", "effective", "successful"]
        for indicator in negative_indicators:
            if indicator in chunk_lower:
                features["sentiment_indicators"].append(("negative", indicator))
        for indicator in positive_indicators:
            if indicator in chunk_lower:
                features["sentiment_indicators"].append(("positive", indicator))
        return features

    def _classify_chunk_type(self, chunk: str, features: Dict[str, Any]) -> str:
        """åˆ†ç±»chunkç±»å‹"""
        chunk_lower = chunk.lower()
        if any(word in chunk_lower for word in ["risk factor", "item 1a", "risk management"]):
            return "risk_disclosure"
        elif any(word in chunk_lower for word in ["financial statement", "balance sheet", "income statement"]):
            return "financial_data"
        elif any(word in chunk_lower for word in ["internal control", "sox", "compliance"]):
            return "compliance"
        elif any(word in chunk_lower for word in ["management discussion", "md&a", "outlook"]):
            return "management_analysis"
        elif features["regulatory_mentions"] > 0:
            return "regulatory"
        elif features["risk_signals"] > 2:
            return "risk_assessment"
        elif features["financial_terms"] > 1:
            return "financial_metrics"
        else:
            return "general"

    def _calculate_importance_score(self, chunk: str, features: Dict[str, Any]) -> float:
        """è®¡ç®—chunké‡è¦æ€§åˆ†æ•°"""
        score = 0.0
        score += min(len(chunk.split()) / 100, 1.0) * 0.2
        score += min(features["risk_signals"] / 10, 1.0) * 0.3
        score += min(features["financial_terms"] / 10, 1.0) * 0.2
        score += min(features["regulatory_mentions"] / 5, 1.0) * 0.2
        score += min(len(features["entities"]) / 10, 1.0) * 0.1
        return min(score, 1.0)

    async def _enhance_chunks_with_entities(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å®ä½“ä¿¡æ¯å¢å¼ºchunks"""
        enhanced = []
        for chunk_data in chunks:
            chunk = chunk_data["content"]
            metadata = chunk_data["metadata"]
            annotated_chunk = await self._annotate_entities(chunk)
            chunk_summary = await self._generate_chunk_summary(chunk)
            keywords = self._extract_chunk_keywords(chunk)
            enhanced.append({
                "content": annotated_chunk,
                "metadata": {**metadata, "summary": chunk_summary, "keywords": keywords, "enhanced_at": datetime.now().isoformat()}
            })
        return enhanced

    async def _annotate_entities(self, text: str) -> str:
        """æ ‡æ³¨å®ä½“"""
        annotated = text
        for pattern in self.risk_entities["monetary_patterns"]:
            annotated = re.sub(pattern, r'<MONEY>\g<0></MONEY>', annotated, flags=re.IGNORECASE)
        for pattern in self.risk_entities["percentage_patterns"]:
            annotated = re.sub(pattern, r'<PERCENT>\g<0></PERCENT>', annotated)
        return annotated

    async def _generate_chunk_summary(self, chunk: str) -> str:
        """ç”Ÿæˆchunkæ‘˜è¦"""
        if len(chunk) < 200:
            return chunk[:100] + "..."
        summary_prompt = ChatPromptTemplate.from_template("""
        è¯·ä¸ºä»¥ä¸‹é‡‘èæ–‡æ¡£ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ï¼š
        æ–‡æ¡£ç‰‡æ®µï¼š{chunk}
        æ‘˜è¦ï¼š
        """)
        try:
            chain = summary_prompt | self.llm | StrOutputParser()
            summary = await asyncio.to_thread(chain.invoke, {"chunk": chunk[:1000]})
            return summary.strip()
        except Exception as e:
            print(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return chunk[:100] + "..."

    def _extract_chunk_keywords(self, chunk: str) -> List[str]:
        """æå–chunkå…³é”®è¯"""
        keywords = []
        chunk_lower = chunk.lower()
        for category, word_list in self.financial_keywords.items():
            for word in word_list:
                if word in chunk_lower:
                    keywords.append(word)
        if self.nlp:
            doc = self.nlp(chunk)
            for ent in doc.ents:
                if ent.label_ in ["ORG", "MONEY", "PERCENT", "LAW"]:
                    keywords.append(ent.text.lower())
        return list(set(keywords))[:10]

    async def advanced_retrieve(self, query: str, vectorstore: FAISS, retrieval_strategy: str = "hybrid") -> List[Document]:
        """é«˜çº§æ£€ç´¢ç­–ç•¥"""
        print(f"ğŸ” å¼€å§‹é«˜çº§æ£€ç´¢ï¼š{query}")
        enhanced_queries = await self._enhance_query(query)
        if retrieval_strategy == "hybrid":
            documents = await self._hybrid_retrieval(enhanced_queries, vectorstore)
        elif retrieval_strategy == "semantic":
            documents = await self._semantic_retrieval(enhanced_queries, vectorstore)
        elif retrieval_strategy == "keyword":
            documents = await self._keyword_retrieval(enhanced_queries, vectorstore)
        else:
            documents = await self._ensemble_retrieval(enhanced_queries, vectorstore)
        if self.config["use_reranking"]:
            documents = await self._intelligent_reranking(query, documents)
        if self.config["use_compression"]:
            documents = await self._compress_context(query, documents)
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
        return documents

    async def _enhance_query(self, query: str) -> List[str]:
        """æŸ¥è¯¢å¢å¼ºå’Œæ‰©å±•"""
        enhanced_queries = [query]
        synonyms = await self._get_financial_synonyms(query)
        if synonyms:
            enhanced_queries.extend(synonyms[:3])
        if self.config["enable_multi_query"]:
            multi_queries = await self._generate_multi_perspective_queries(query)
            enhanced_queries.extend(multi_queries[:2])
        entities = self._extract_query_entities(query)
        if entities:
            entity_expanded_query = await self._expand_with_entities(query, entities)
            enhanced_queries.append(entity_expanded_query)
        return enhanced_queries

    async def _get_financial_synonyms(self, query: str) -> List[str]:
        """è·å–é‡‘èé¢†åŸŸåŒä¹‰è¯"""
        financial_synonyms = {"risk": ["exposure", "hazard", "vulnerability", "threat"], "loss": ["deficit", "shortfall", "decline", "reduction"], "profit": ["earnings", "income", "revenue", "gains"], "compliance": ["adherence", "conformity", "regulation"], "audit": ["review", "examination", "assessment", "evaluation"]}
        synonyms = []
        query_lower = query.lower()
        for word, syn_list in financial_synonyms.items():
            if word in query_lower:
                for synonym in syn_list:
                    synonym_query = query_lower.replace(word, synonym)
                    synonyms.append(synonym_query)
        return synonyms

    async def _generate_multi_perspective_queries(self, query: str) -> List[str]:
        """ç”Ÿæˆå¤šè§’åº¦æŸ¥è¯¢"""
        multi_query_prompt = ChatPromptTemplate.from_template("""
        ä½œä¸ºé‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ2ä¸ªä¸åŒè§’åº¦çš„ç›¸å…³é—®é¢˜ï¼Œç”¨äºæ£€ç´¢10-Kæ–‡æ¡£ï¼š
        åŸå§‹æŸ¥è¯¢ï¼š{query}
        è¯·ç”Ÿæˆï¼š
        1. ä¸€ä¸ªæ›´å…·ä½“çš„æŸ¥è¯¢
        2. ä¸€ä¸ªæ›´å¹¿æ³›çš„ç›¸å…³æŸ¥è¯¢
        æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼Œä¸è¦ç¼–å·
        """)
        try:
            chain = multi_query_prompt | self.llm | StrOutputParser()
            result = await asyncio.to_thread(chain.invoke, {"query": query})
            queries = [line.strip() for line in result.split('\n') if line.strip()]
            return queries[:2]
        except Exception as e:
            print(f"å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return []

    def _extract_query_entities(self, query: str) -> List[Dict[str, str]]:
        """æå–æŸ¥è¯¢ä¸­çš„å®ä½“"""
        entities = []
        if self.nlp:
            doc = self.nlp(query)
            for ent in doc.ents:
                entities.append({"text": ent.text, "label": ent.label_, "start": ent.start_char, "end": ent.end_char})
        return entities

    async def _expand_with_entities(self, query: str, entities: List[Dict[str, str]]) -> str:
        """åŸºäºå®ä½“æ‰©å±•æŸ¥è¯¢"""
        expanded_terms = []
        for entity in entities:
            if entity["label"] == "ORG":
                expanded_terms.append(f"{entity['text']} company")
            elif entity["label"] == "MONEY":
                expanded_terms.append(f"financial impact {entity['text']}")
        if expanded_terms:
            return f"{query} {' '.join(expanded_terms)}"
        return query

    async def _hybrid_retrieval(self, queries: List[str], vectorstore: FAISS) -> List[Document]:
        """æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼æ€§ + å…³é”®è¯åŒ¹é…"""
        all_docs = []
        for query in queries:
            vector_docs = await asyncio.to_thread(vectorstore.similarity_search, query, k=self.config["retrieval_k"])
            keyword_filtered = self._keyword_filter(query, vector_docs)
            all_docs.extend(keyword_filtered)
        unique_docs = self._deduplicate_documents(all_docs)
        return unique_docs[:self.config["rerank_top_k"] * 2]

    def _keyword_filter(self, query: str, docs: List[Document]) -> List[Document]:
        """åŸºäºå…³é”®è¯è¿‡æ»¤å’Œè¯„åˆ†æ–‡æ¡£"""
        query_terms = set(query.lower().split())
        filtered_docs = []
        for doc in docs:
            content_lower = doc.page_content.lower()
            keyword_score = sum(1 for term in query_terms if term in content_lower) / len(query_terms)
            financial_score = 0
            for category, terms in self.financial_keywords.items():
                for term in terms:
                    if term in content_lower and any(qt in term for qt in query_terms):
                        financial_score += 1
            total_score = keyword_score * 0.7 + min(financial_score / 10, 1.0) * 0.3
            if total_score > self.config["similarity_threshold"] * 0.5:
                doc.metadata["keyword_score"] = keyword_score
                doc.metadata["financial_score"] = financial_score
                doc.metadata["total_score"] = total_score
                filtered_docs.append(doc)
        return sorted(filtered_docs, key=lambda x: x.metadata.get("total_score", 0), reverse=True)

    def _deduplicate_documents(self, docs: List[Document]) -> List[Document]:
        """æ–‡æ¡£å»é‡"""
        seen_content = set()
        unique_docs = []
        for doc in docs:
            content_signature = doc.page_content[:100]
            if content_signature not in seen_content:
                seen_content.add(content_signature)
                unique_docs.append(doc)
        return unique_docs

    async def _intelligent_reranking(self, query: str, documents: List[Document]) -> List[Document]:
        """æ™ºèƒ½é‡æ’åº"""
        if len(documents) <= self.config["rerank_top_k"]:
            return documents
        reranked = []
        for doc in documents:
            rerank_score = await self._calculate_rerank_score(query, doc)
            doc.metadata["rerank_score"] = rerank_score
            reranked.append(doc)
        reranked.sort(key=lambda x: x.metadata.get("rerank_score", 0), reverse=True)
        return reranked[:self.config["rerank_top_k"]]

    async def _calculate_rerank_score(self, query: str, document: Document) -> float:
        """è®¡ç®—é‡æ’åºåˆ†æ•°"""
        score = 0.0
        if self.sentence_model:
            try:
                query_embedding = self.sentence_model.encode([query])
                doc_embedding = self.sentence_model.encode([document.page_content[:500]])
                semantic_similarity = util.cos_sim(query_embedding, doc_embedding)[0][0].item()
                score += semantic_similarity * 0.4
            except Exception:
                score += 0.2
        keyword_score = document.metadata.get("keyword_score", 0)
        score += keyword_score * 0.3
        importance_score = document.metadata.get("importance_score", 0.5)
        score += importance_score * 0.2
        chunk_type = document.metadata.get("chunk_type", "general")
        type_weights = {"risk_disclosure": 1.0, "compliance": 0.9, "financial_data": 0.8, "management_analysis": 0.7, "regulatory": 0.8, "risk_assessment": 0.9, "general": 0.5}
        score += type_weights.get(chunk_type, 0.5) * 0.1
        return min(score, 1.0)

    async def _compress_context(self, query: str, documents: List[Document]) -> List[Document]:
        """ä¸Šä¸‹æ–‡å‹ç¼©"""
        if len(documents) <= 3:
            return documents
        compression_prompt = ChatPromptTemplate.from_template("""
        ä½œä¸ºé‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µä¸­æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚
        ä¿æŒåŸæ–‡çš„é‡è¦ç»†èŠ‚ï¼Œä½†å»é™¤å†—ä½™å†…å®¹ã€‚
        æŸ¥è¯¢ï¼š{query}
        æ–‡æ¡£å†…å®¹ï¼š{context}
        è¯·æå–æœ€ç›¸å…³çš„ä¿¡æ¯ï¼ˆä¿æŒåŸæ–‡è¡¨è¿°ï¼‰ï¼š
        """)
        compressed_docs = []
        for doc in documents:
            try:
                chain = compression_prompt | self.llm | StrOutputParser()
                compressed_content = await asyncio.to_thread(chain.invoke, {"query": query, "context": doc.page_content[:1500]})
                compressed_doc = Document(page_content=compressed_content.strip(), metadata={**doc.metadata, "compressed": True, "original_length": len(doc.page_content), "compressed_length": len(compressed_content)})
                compressed_docs.append(compressed_doc)
            except Exception as e:
                print(f"å‹ç¼©å¤±è´¥ï¼Œä¿ç•™åŸæ–‡æ¡£: {e}")
                compressed_docs.append(doc)
        return compressed_docs

    async def intelligent_qa(self, query: str, vectorstore: FAISS, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
        start_time = time.time()
        processed_query = await self._preprocess_query(query, conversation_history)
        relevant_docs = await self.advanced_retrieve(processed_query, vectorstore)
        if self._requires_multi_hop_reasoning(processed_query):
            relevant_docs = await self._multi_hop_retrieval(processed_query, vectorstore, relevant_docs)
        qa_prompt = await self._generate_dynamic_prompt(processed_query, relevant_docs)
        answer = await self._generate_answer(qa_prompt, processed_query, relevant_docs)
        final_answer = await self._post_process_answer(answer, processed_query, relevant_docs)
        confidence_score = self._calculate_confidence(processed_query, relevant_docs, final_answer)
        citations = self._generate_citations(relevant_docs)
        explanation = await self._generate_explanation(processed_query, final_answer, relevant_docs)
        processing_time = time.time() - start_time
        return {
            "query": query,
            "answer": final_answer,
            "confidence_score": confidence_score,
            "citations": citations,
            "explanation": explanation,
            "relevant_documents": [{"content": doc.page_content[:200] + "...", "metadata": doc.metadata, "relevance_score": doc.metadata.get("rerank_score", 0)} for doc in relevant_docs],
            "processing_time": processing_time,
            "retrieval_strategy": "advanced_hybrid",
            "documents_retrieved": len(relevant_docs)
        }

    async def _preprocess_query(self, query: str, conversation_history: List[Dict[str, str]] = None) -> str:
        """æŸ¥è¯¢é¢„å¤„ç†"""
        processed = query.strip()
        if conversation_history:
            context = self._extract_conversation_context(conversation_history)
            if context:
                processed = f"{context} {processed}"
        expanded = await self._expand_financial_terms(processed)
        return expanded

    def _extract_conversation_context(self, history: List[Dict[str, str]]) -> str:
        """æå–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if not history or len(history) == 0:
            return ""
        recent_context = []
        for item in history[-2:]:
            if item.get("role") == "user":
                recent_context.append(f"ä¹‹å‰é—®é¢˜ï¼š{item.get('content', '')}")
        return " ".join(recent_context)

    async def _expand_financial_terms(self, query: str) -> str:
        """æ‰©å±•é‡‘èæœ¯è¯­"""
        query_lower = query.lower()
        expansions = []
        if "risk" in query_lower:
            risk_context = [rt for rt in self.financial_keywords["risk_types"] if any(word in query_lower for word in rt.split())]
            if risk_context:
                expansions.extend(risk_context[:3])
        for regulation in self.financial_keywords["regulations"]:
            if regulation.lower() in query_lower:
                expansions.append(f"{regulation} compliance")
        if expansions:
            return f"{query} ({' OR '.join(expansions)})"
        return query

    def _requires_multi_hop_reasoning(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè·³æ¨ç†"""
        multi_hop_indicators = ["compare", "relationship", "impact", "cause", "effect", "correlation", "trend", "change", "difference", "connection"]
        query_lower = query.lower()
        return any(indicator in query_lower for indicator in multi_hop_indicators)

    async def _multi_hop_retrieval(self, query: str, vectorstore: FAISS, initial_docs: List[Document]) -> List[Document]:
        """å¤šè·³æ£€ç´¢"""
        key_concepts = await self._extract_key_concepts_from_docs(initial_docs)
        additional_docs = []
        for concept in key_concepts[:3]:
            concept_query = f"{concept} {query}"
            concept_docs = await asyncio.to_thread(vectorstore.similarity_search, concept_query, k=3)
            additional_docs.extend(concept_docs)
        all_docs = initial_docs + additional_docs
        return self._deduplicate_documents(all_docs)[:self.config["rerank_top_k"]]

    async def _extract_key_concepts_from_docs(self, docs: List[Document]) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–å…³é”®æ¦‚å¿µ"""
        key_concepts = []
        for doc in docs[:3]:
            content_lower = doc.page_content.lower()
            for category, terms in self.financial_keywords.items():
                for term in terms:
                    if term in content_lower:
                        key_concepts.append(term)
        concept_counts = Counter(key_concepts)
        return [concept for concept, count in concept_counts.most_common(5)]

    async def _generate_dynamic_prompt(self, query: str, documents: List[Document]) -> ChatPromptTemplate:
        """åŠ¨æ€ç”Ÿæˆæç¤ºè¯"""
        query_type = self._classify_query_type(query)
        doc_types = [doc.metadata.get("chunk_type", "general") for doc in documents]
        dominant_doc_type = Counter(doc_types).most_common(1)[0][0] if doc_types else "general"
        prompt_template = self._select_prompt_template(query_type, dominant_doc_type)
        return ChatPromptTemplate.from_template(prompt_template)

    def _classify_query_type(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        if any(word in query_lower for word in ["risk", "exposure", "threat", "vulnerability"]):
            return "risk_analysis"
        elif any(word in query_lower for word in ["compliance", "regulation", "sox", "sec"]):
            return "compliance"
        elif any(word in query_lower for word in ["financial", "revenue", "profit", "loss", "metric"]):
            return "financial_analysis"
        elif any(word in query_lower for word in ["compare", "difference", "vs", "versus"]):
            return "comparison"
        elif any(word in query_lower for word in ["trend", "change", "over time", "historical"]):
            return "trend_analysis"
        else:
            return "general"

    def _select_prompt_template(self, query_type: str, doc_type: str) -> str:
        """é€‰æ‹©é€‚åˆçš„æç¤ºè¯æ¨¡æ¿"""
        base_template = """
        æ‚¨æ˜¯ä¸€ä½èµ„æ·±çš„é‡‘èé£é™©åˆ†æä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„10-Kæ–‡æ¡£åˆ†æç»éªŒã€‚
        è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
        ç›¸å…³æ–‡æ¡£ï¼š{context}
        ç”¨æˆ·é—®é¢˜ï¼š{question}
        å›ç­”è¦æ±‚ï¼š
        """
        if query_type == "risk_analysis":
            base_template += "- é‡ç‚¹åˆ†æé£é™©ç±»å‹ã€ä¸¥é‡ç¨‹åº¦å’Œæ½œåœ¨å½±å“\n- å¼•ç”¨å…·ä½“çš„é£é™©å› ç´ å’Œæ•°æ®\n- è¯„ä¼°é£é™©çš„å¯æ§æ€§å’Œç¼“è§£æªæ–½\n- å¦‚æœæ¶‰åŠç›‘ç®¡é£é™©ï¼Œè¯·æ˜ç¡®ç›¸å…³æ³•è§„æ¡æ¬¾\n"
        elif query_type == "compliance":
            base_template += "- æ˜ç¡®æŒ‡å‡ºç›¸å…³çš„ç›‘ç®¡è¦æ±‚å’Œåˆè§„çŠ¶æ€\n- å¼•ç”¨å…·ä½“çš„æ³•è§„æ¡æ¬¾ï¼ˆå¦‚SOX 404ã€SEC Item 105ç­‰ï¼‰\n- åˆ†æåˆè§„ç¼ºé™·çš„ä¸¥é‡æ€§å’Œæ•´æ”¹è¦æ±‚\n- è¯„ä¼°å¯¹ä¸šåŠ¡è¿è¥çš„å½±å“\n"
        elif query_type == "financial_analysis":
            base_template += "- æä¾›å…·ä½“çš„è´¢åŠ¡æ•°æ®å’ŒæŒ‡æ ‡\n- åˆ†æè´¢åŠ¡è¶‹åŠ¿å’Œå˜åŒ–åŸå› \n- è¯„ä¼°å¯¹å…¬å¸è´¢åŠ¡å¥åº·çŠ¶å†µçš„å½±å“\n- å¦‚æœ‰åŒæ¯”æ•°æ®ï¼Œè¯·è¿›è¡Œå¯¹æ¯”åˆ†æ\n"
        elif query_type == "comparison":
            base_template += "- è¿›è¡Œè¯¦ç»†çš„å¯¹æ¯”åˆ†æ\n- çªå‡ºæ˜¾ç¤ºå…³é”®å·®å¼‚å’Œç›¸ä¼¼ç‚¹\n- åˆ†æå·®å¼‚çš„åŸå› å’Œå½±å“\n- æä¾›æ•°æ®æ”¯æŒçš„ç»“è®º\n"
        else:
            base_template += "- æä¾›å‡†ç¡®ã€å…·ä½“çš„å›ç­”\n- å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£å†…å®¹ä½œä¸ºä¾æ®\n- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜\n- ä¿æŒå®¢è§‚å’Œä¸“ä¸šçš„åˆ†æè§’åº¦\n"
        base_template += """
        æ³¨æ„äº‹é¡¹ï¼š
        - å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        - å¼•ç”¨æ—¶è¯·ä¿æŒåŸæ–‡çš„å‡†ç¡®æ€§
        - æä¾›å¯ä¿¡åº¦è¯„ä¼°ï¼ˆé«˜/ä¸­/ä½ï¼‰
        - å¦‚éœ€è¦ï¼Œå¯ä»¥æå‡ºè¿›ä¸€æ­¥åˆ†æçš„å»ºè®®
        å›ç­”ï¼š
        """
        return base_template

    async def _generate_answer(self, prompt: ChatPromptTemplate, query: str, documents: List[Document]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([f"æ–‡æ¡£ {i+1}:\n{doc.page_content}" for i, doc in enumerate(documents[:5])])
        try:
            chain = prompt | self.llm | StrOutputParser()
            answer = await asyncio.to_thread(chain.invoke, {"context": context, "question": query})
            return answer.strip()
        except Exception as e:
            print(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"

    async def _post_process_answer(self, answer: str, query: str, documents: List[Document]) -> str:
        """ç­”æ¡ˆåå¤„ç†"""
        verified_answer = await self._verify_facts(answer, documents)
        formatted_answer = self._format_answer(verified_answer)
        if self._requires_disclaimer(query):
            formatted_answer += "\n\nâš ï¸ å…è´£å£°æ˜ï¼šæœ¬åˆ†æåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œä»…ä¾›å‚è€ƒã€‚å…·ä½“å†³ç­–è¯·å’¨è¯¢ä¸“ä¸šé¡¾é—®ã€‚"
        return formatted_answer

    async def _verify_facts(self, answer: str, documents: List[Document]) -> str:
        """éªŒè¯ç­”æ¡ˆä¸­çš„äº‹å®"""
        numbers = re.findall(r'\d+(?:\.\d+)?%?', answer)
        all_doc_content = " ".join([doc.page_content for doc in documents])
        verified_numbers = [num for num in numbers if num in all_doc_content]
        verification_ratio = len(verified_numbers) / len(numbers) if numbers else 1.0
        if verification_ratio < 0.5:
            answer = f"âš ï¸ éƒ¨åˆ†ä¿¡æ¯å¯èƒ½éœ€è¦è¿›ä¸€æ­¥éªŒè¯\n\n{answer}"
        return answer

    def _format_answer(self, answer: str) -> str:
        """æ ¼å¼åŒ–ç­”æ¡ˆ"""
        formatted = answer
        if len(formatted) > 500:
            formatted = re.sub(r'ã€‚([^ã€‚]{100,})', r'ã€‚\n\n\1', formatted)
        return formatted

    def _requires_disclaimer(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ å…è´£å£°æ˜"""
        disclaimer_triggers = ["investment", "invest", "buy", "sell", "recommendation", "advice", "should", "suggest", "recommend"]
        query_lower = query.lower()
        return any(trigger in query_lower for trigger in disclaimer_triggers)

    def _calculate_confidence(self, query: str, documents: List[Document], answer: str) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        confidence = 0.0
        doc_quality = np.mean([doc.metadata.get("rerank_score", 0.5) for doc in documents])
        confidence += doc_quality * 0.4
        answer_completeness = min(len(answer) / 500, 1.0)
        confidence += answer_completeness * 0.2
        query_terms = set(query.lower().split())
        answer_terms = set(answer.lower().split())
        keyword_match = len(query_terms.intersection(answer_terms)) / len(query_terms)
        confidence += keyword_match * 0.2
        doc_count_score = min(len(documents) / 5, 1.0)
        confidence += doc_count_score * 0.1
        has_specific_data = bool(re.search(r'\d+(?:\.\d+)?[%$]?', answer))
        if has_specific_data:
            confidence += 0.1
        return min(confidence, 1.0)

    def _generate_citations(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, doc in enumerate(documents):
            citation = {"id": i + 1, "content_preview": doc.page_content[:150] + "...", "relevance_score": doc.metadata.get("rerank_score", 0), "document_type": doc.metadata.get("chunk_type", "unknown"), "metadata": {k: v for k, v in doc.metadata.items() if k in ["document_index", "chunk_index", "importance_score"]}}
            citations.append(citation)
        return citations

    async def _generate_explanation(self, query: str, answer: str, documents: List[Document]) -> str:
        """ç”Ÿæˆå›ç­”è§£é‡Š"""
        explanation_prompt = ChatPromptTemplate.from_template("""
        è¯·ä¸ºä»¥ä¸‹é—®ç­”å¯¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„è§£é‡Šï¼Œè¯´æ˜ç­”æ¡ˆçš„ä¾æ®å’Œæ¨ç†è¿‡ç¨‹ï¼š
        é—®é¢˜ï¼š{query}
        ç­”æ¡ˆï¼š{answer}
        ä½¿ç”¨çš„æ–‡æ¡£æ•°é‡ï¼š{doc_count}
        è§£é‡Šï¼ˆä¸è¶…è¿‡100å­—ï¼‰ï¼š
        """)
        try:
            chain = explanation_prompt | self.llm | StrOutputParser()
            explanation = await asyncio.to_thread(chain.invoke, {"query": query, "answer": answer[:500], "doc_count": len(documents)})
            return explanation.strip()
        except Exception as e:
            print(f"è§£é‡Šç”Ÿæˆå¤±è´¥: {e}")
            return f"åŸºäº {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆçš„ç­”æ¡ˆ"

async def test_advanced_rag():
    """æµ‹è¯•é«˜çº§RAGç³»ç»Ÿ"""
    rag_service = AdvancedRAGService()
    test_documents = [
        "å…¬å¸é¢ä¸´çš„ä¸»è¦é£é™©åŒ…æ‹¬å¸‚åœºé£é™©ã€ä¿¡ç”¨é£é™©å’Œæ“ä½œé£é™©ã€‚å¸‚åœºé£é™©ä¸»è¦æ¥è‡ªåˆ©ç‡å˜åŒ–å’Œæ±‡ç‡æ³¢åŠ¨ï¼Œé¢„è®¡å¯èƒ½å¯¼è‡´å¹´åº¦æ”¶ç›Šä¸‹é™5-10%ã€‚",
        "æ ¹æ®SOX 404æ¡æ¬¾è¦æ±‚ï¼Œå…¬å¸å»ºç«‹äº†å†…éƒ¨æ§åˆ¶åˆ¶åº¦ã€‚ä½†åœ¨2023å¹´åº¦å®¡è®¡ä¸­å‘ç°äº†ä¸€é¡¹é‡å¤§ç¼ºé™·ï¼Œæ¶‰åŠæ”¶å…¥ç¡®è®¤æµç¨‹ã€‚",
        "å…¬å¸çš„æµåŠ¨æ€§æ¯”ç‡ä¸º1.5ï¼Œå€ºåŠ¡æƒç›Šæ¯”ä¸º0.8ã€‚ç°é‡‘æµé‡è¡¨æ˜¾ç¤ºç»è¥æ´»åŠ¨ç°é‡‘æµä¸ºæ­£ï¼Œä½†æŠ•èµ„æ´»åŠ¨ç°é‡‘æµä¸ºè´Ÿã€‚"
    ]
    test_metadata = [{"document_type": "10-K", "section": "Risk Factors"}, {"document_type": "10-K", "section": "Internal Controls"}, {"document_type": "10-K", "section": "Financial Statements"}]
    print("ğŸ”„ æ„å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = await rag_service.build_enhanced_vectorstore(test_documents, test_metadata, save_path="test_vectorstore")
    test_queries = ["å…¬å¸é¢ä¸´å“ªäº›ä¸»è¦é£é™©ï¼Ÿ", "SOXåˆè§„çŠ¶å†µå¦‚ä½•ï¼Ÿ", "å…¬å¸çš„è´¢åŠ¡çŠ¶å†µæ˜¯å¦å¥åº·ï¼Ÿ", "å¸‚åœºé£é™©å¯¹å…¬å¸çš„å½±å“æœ‰å¤šå¤§ï¼Ÿ"]
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ï¼š{query}")
        result = await rag_service.intelligent_qa(query, vectorstore)
        print(f"ç­”æ¡ˆï¼š{result['answer']}")
        print(f"ç½®ä¿¡åº¦ï¼š{result['confidence_score']:.2f}")
        print(f"æ£€ç´¢æ–‡æ¡£æ•°ï¼š{result['documents_retrieved']}")
        print(f"å¤„ç†æ—¶é—´ï¼š{result['processing_time']:.2f}ç§’")

if __name__ == "__main__":
    asyncio.run(test_advanced_rag())