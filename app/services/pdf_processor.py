# services/pdf_processor.py
import json
import os
import time
import logging
import asyncio
import aiofiles
import re
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from ..utils.config import Config
from ..utils.rag_config import RAGConfig
from .rag_service import UnifiedRAGService

class PDFProcessorService:
    def __init__(self):
        # ä½¿ç”¨RAGé…ç½®
        rag_config = RAGConfig.get_config()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=rag_config["chunk_size"],
            chunk_overlap=rag_config["chunk_overlap"],
            separators=["\n\n", "\n", ". ", "ã€‚", "ï¼›", ";", ":", "ï¼š", " "]
        )
        self.rag_service = UnifiedRAGService(config=rag_config)

    async def process_pdf(self, file_path: str, document_id: str, metadata: dict, save_vectorstore: bool = True, vectorstore_path: str = None) -> dict:
        """
        å¤„ç†PDFæ–‡æ¡£ï¼Œå°†å…¨æ–‡åˆ†å‰²æˆå¤šä¸ªæ®µè½å¹¶æ„å»ºRAGå‘é‡æ•°æ®åº“
        - å®Œæ•´è§£æPDFå†…å®¹ï¼Œä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯
        - æ™ºèƒ½åˆ†å‰²æˆé€‚åˆRAGçš„æ®µè½
        - æ„å»ºå‘é‡æ•°æ®åº“ç”¨äºåç»­æŸ¥è¯¢
        """
        try:
            start_time = time.time()
            logging.info(f"ğŸ”„ å¼€å§‹å¤„ç†PDF: {file_path}")

            # 1. åŠ è½½PDFæ–‡æ¡£
            loader = PyPDFLoader(file_path)
            pages = await asyncio.to_thread(loader.load)
            
            # 2. åˆå¹¶æ‰€æœ‰é¡µé¢å†…å®¹
            full_text = ""
            page_info = []
            for i, page in enumerate(pages):
                page_content = page.page_content.strip()
                if page_content:  # åªå¤„ç†éç©ºé¡µé¢
                    full_text += f"\n\n{page_content}"
                    page_info.append({
                        "page_number": i + 1,
                        "content_length": len(page_content),
                        "has_content": bool(page_content.strip())
                    })
            
            logging.info(f"ğŸ“„ PDFåŠ è½½å®Œæˆ: {len(pages)}é¡µ, æ€»å­—ç¬¦æ•°: {len(full_text)}")

            # 3. æ™ºèƒ½æ®µè½åˆ†å‰² - ä¿æŒå®Œæ•´æ€§
            documents = []
            document_metadata = []
            
            # ä½¿ç”¨å¢å¼ºçš„æ–‡æœ¬åˆ†å‰²å™¨è¿›è¡Œåˆ†å‰²
            chunks = self.text_splitter.split_text(full_text)
            
            logging.info(f"ğŸ“ æ™ºèƒ½åˆ†å‰²å®Œæˆ: {len(chunks)}ä¸ªæ®µè½")
            
            # 4. ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                # ç¡®å®šæ®µè½æ‰€å±é¡µé¢
                page_num = self._determine_page_number(chunk, pages)
                
                # è¯†åˆ«æ®µè½ç±»å‹å’Œé‡è¦æ€§
                section_name = self._identify_section(chunk)
                chunk_importance = self._calculate_chunk_importance(chunk)
                
                # æå–å…³é”®ä¿¡æ¯
                key_entities = self._extract_key_entities(chunk)
                
                chunk_metadata = {
                    **metadata,
                    "chunk_index": i,
                    "page_number": page_num,
                    "section_name": section_name,
                    "importance_score": chunk_importance,
                    "word_count": len(chunk.split()),
                    "char_count": len(chunk),
                    "key_entities": key_entities,
                    "has_financial_data": self._has_financial_data(chunk),
                    "has_risk_content": self._has_risk_content(chunk),
                    "document_id": document_id,
                    "processed_at": time.time()
                }
                
                documents.append(chunk)
                document_metadata.append(chunk_metadata)

            # 5. ä¿å­˜å¤„ç†åçš„æ•°æ®
            processed_data = {
                "document_id": document_id,
                "company_name": metadata.get("company", "Unknown"),
                "file_name": os.path.basename(file_path),
                "total_pages": len(pages),
                "total_chunks": len(documents),
                "processing_time": time.time() - start_time,
                "paragraphs": [
                    {"content": doc, "metadata": meta} 
                    for doc, meta in zip(documents, document_metadata)
                ],
                "page_info": page_info,
                "statistics": {
                    "total_words": sum(len(doc.split()) for doc in documents),
                    "total_chars": sum(len(doc) for doc in documents),
                    "avg_chunk_size": sum(len(doc) for doc in documents) / len(documents) if documents else 0,
                    "sections_identified": len(set(meta.get("section_name", "general") for meta in document_metadata)),
                    "high_importance_chunks": sum(1 for meta in document_metadata if meta.get("importance_score", 0) > 0.7)
                }
            }
            
            # ä¿å­˜JSONæ–‡ä»¶
            storage_path = Path(Config.STORAGE_PATH) / f"{document_id}.json"
            storage_path.parent.mkdir(parents=True, exist_ok=True)
            async with aiofiles.open(storage_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(processed_data, indent=2, ensure_ascii=False))

            logging.info(f"ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ: {len(documents)}ä¸ªæ®µè½ -> {storage_path}")

            # 6. æ„å»ºRAGå‘é‡æ•°æ®åº“
            if save_vectorstore:
                vectorstore_path = vectorstore_path or f"{Config.STORAGE_PATH}/{document_id}_vectorstore"
                
                logging.info(f"ğŸ” å¼€å§‹æ„å»ºRAGå‘é‡æ•°æ®åº“...")
                vectorstore = await self.rag_service.build_enhanced_vectorstore(
                    documents=documents,
                    document_metadata=document_metadata,
                    save_path=vectorstore_path
                )
                
                processed_data["vectorstore_path"] = vectorstore_path
                processed_data["vectorstore_built"] = True
                
                logging.info(f"âœ… RAGå‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ: {vectorstore_path}")
            else:
                processed_data["vectorstore_built"] = False

            total_time = time.time() - start_time
            logging.info(f"ğŸ‰ PDFå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logging.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {len(documents)}ä¸ªæ®µè½, {processed_data['statistics']['total_words']}ä¸ªå•è¯")
            
            return processed_data

        except FileNotFoundError as e:
            logging.error(f"âŒ PDFæ–‡ä»¶æœªæ‰¾åˆ°: {file_path}, é”™è¯¯: {e}")
            raise
        except Exception as e:
            logging.error(f"âŒ å¤„ç†PDF {file_path} æ—¶å‡ºé”™: {e}")
            raise

    async def query_pdf(self, document_id: str, query: str, vectorstore_path: str = None, conversation_history: list = None) -> dict:
        """
        å¯¹å·²å¤„ç†çš„PDFæ‰§è¡ŒRAGæ™ºèƒ½æŸ¥è¯¢
        - ä½¿ç”¨å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ®µè½
        - é€šè¿‡LLMç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆ
        - æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
        """
        try:
            logging.info(f"ğŸ” å¼€å§‹RAGæŸ¥è¯¢: {query[:50]}...")
            
            # åŠ è½½å‘é‡æ•°æ®åº“
            vectorstore_path = vectorstore_path or f"{Config.STORAGE_PATH}/{document_id}_vectorstore"
            
            if not os.path.exists(vectorstore_path):
                raise FileNotFoundError(f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {vectorstore_path}")
            
            vectorstore = await asyncio.to_thread(
                FAISS.load_local,
                vectorstore_path,
                self.rag_service.embedding_model
            )

            # æ‰§è¡Œæ™ºèƒ½é—®ç­”
            result = await self.rag_service.intelligent_qa(
                query=query, 
                vectorstore=vectorstore,
                conversation_history=conversation_history
            )
            
            # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
            result["document_id"] = document_id
            result["vectorstore_path"] = vectorstore_path
            
            logging.info(f"âœ… RAGæŸ¥è¯¢å®Œæˆ: '{query[:30]}...', æ£€ç´¢åˆ° {result['documents_retrieved']} ä¸ªç›¸å…³æ®µè½")
            return result

        except Exception as e:
            logging.error(f"âŒ RAGæŸ¥è¯¢å¤±è´¥ {document_id}: {e}")
            raise
    
    async def batch_query_pdf(self, document_id: str, queries: list, vectorstore_path: str = None) -> list:
        """æ‰¹é‡æŸ¥è¯¢å¤„ç†"""
        results = []
        for query in queries:
            try:
                result = await self.query_pdf(document_id, query, vectorstore_path)
                results.append(result)
            except Exception as e:
                logging.error(f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥ '{query}': {e}")
                results.append({
                    "query": query,
                    "error": str(e),
                    "answer": "æŸ¥è¯¢å¤„ç†å¤±è´¥"
                })
        return results

    def _determine_page_number(self, chunk: str, pages: list) -> int:
        """ç¡®å®šæ®µè½æ‰€å±é¡µé¢"""
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å«chunkå¼€å¤´å†…å®¹çš„é¡µé¢
        chunk_start = chunk[:50].strip()
        for i, page in enumerate(pages):
            if chunk_start in page.page_content:
                return i + 1
        return 1  # é»˜è®¤ç¬¬ä¸€é¡µ
    
    def _identify_section(self, content: str) -> str:
        """è¯†åˆ«æ–‡æ¡£ç« èŠ‚ç±»å‹"""
        content_lower = content.lower()
        
        # é£é™©å› ç´ 
        if any(pattern in content_lower for pattern in ['item 1a', 'risk factor', 'risk management']):
            return "risk_factors"
        
        # è´¢åŠ¡æŠ¥è¡¨
        elif any(pattern in content_lower for pattern in ['balance sheet', 'income statement', 'cash flow', 'financial statement']):
            return "financial_statements"
        
        # ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ
        elif any(pattern in content_lower for pattern in ['management discussion', 'md&a', 'liquidity', 'capital resources']):
            return "management_analysis"
        
        # åˆè§„ä¸å†…æ§
        elif any(pattern in content_lower for pattern in ['internal control', 'sox', 'compliance', 'audit']):
            return "compliance"
        
        # ä¸šåŠ¡æ¦‚è¿°
        elif any(pattern in content_lower for pattern in ['business overview', 'our business', 'products and services']):
            return "business_overview"
        
        # å¸‚åœºä¿¡æ¯
        elif any(pattern in content_lower for pattern in ['market', 'competition', 'industry']):
            return "market_info"
        
        return "general"
    
    def _calculate_chunk_importance(self, chunk: str) -> float:
        """è®¡ç®—æ®µè½é‡è¦æ€§åˆ†æ•°"""
        score = 0.0
        chunk_lower = chunk.lower()
        
        # é£é™©ç›¸å…³å…³é”®è¯
        risk_keywords = ['risk', 'uncertainty', 'threat', 'exposure', 'adverse', 'negative impact']
        for keyword in risk_keywords:
            if keyword in chunk_lower:
                score += 0.15
        
        # è´¢åŠ¡å…³é”®è¯
        financial_keywords = ['revenue', 'profit', 'loss', 'assets', 'liabilities', 'capital', 'investment']
        for keyword in financial_keywords:
            if keyword in chunk_lower:
                score += 0.1
        
        # ç›‘ç®¡å…³é”®è¯
        regulatory_keywords = ['sec', 'regulation', 'compliance', 'sox', 'gaap', 'ifrs']
        for keyword in regulatory_keywords:
            if keyword in chunk_lower:
                score += 0.12
        
        # æ•°å­—ä¿¡æ¯
        if re.search(r'\$[\d,]+', chunk) or re.search(r'\d+%', chunk):
            score += 0.1
        
        # é•¿åº¦å› å­
        word_count = len(chunk.split())
        if 50 <= word_count <= 200:
            score += 0.1
        elif word_count > 200:
            score += 0.05
        
        return min(score, 1.0)
    
    def _extract_key_entities(self, chunk: str) -> dict:
        """æå–å…³é”®å®ä½“"""
        entities = {
            "monetary_amounts": [],
            "percentages": [],
            "organizations": [],
            "dates": []
        }
        
        # æå–è´§å¸é‡‘é¢
        money_pattern = r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:million|billion|thousand|M|B|K))?'
        entities["monetary_amounts"] = re.findall(money_pattern, chunk)[:5]
        
        # æå–ç™¾åˆ†æ¯”
        percent_pattern = r'\d+(?:\.\d+)?%'
        entities["percentages"] = re.findall(percent_pattern, chunk)[:5]
        
        # æå–å¹´ä»½
        year_pattern = r'\b(19|20)\d{2}\b'
        entities["dates"] = re.findall(year_pattern, chunk)[:5]
        
        return entities
    
    def _has_financial_data(self, chunk: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è´¢åŠ¡æ•°æ®"""
        financial_indicators = ['$', '%', 'revenue', 'profit', 'loss', 'assets', 'liabilities', 'million', 'billion']
        chunk_lower = chunk.lower()
        return any(indicator in chunk_lower for indicator in financial_indicators)
    
    def _has_risk_content(self, chunk: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«é£é™©å†…å®¹"""
        risk_indicators = ['risk', 'uncertainty', 'threat', 'exposure', 'adverse', 'challenge', 'volatility']
        chunk_lower = chunk.lower()
        return any(indicator in chunk_lower for indicator in risk_indicators)